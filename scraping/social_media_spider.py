# üìÅ scraping/social_media_spider.py
# Univerz√°ln√≠ spider pro soci√°ln√≠ m√©dia (Reddit, X/Twitter API)
# Dynamicky ƒçte konfiguraci ze sources_config.yaml

import scrapy
import praw
import requests
from datetime import datetime
import os
from dotenv import load_dotenv
from scraping.config_loader import get_config_loader
from scraping.keywords import contains_relevant_keywords


# Naƒçten√≠ .env souboru
load_dotenv()


class RedditSpider(scrapy.Spider):
    """
    Spider pro Reddit API.
    Hled√° p≈ô√≠spƒõvky o sekt√°ch a n√°bo≈æensk√Ωch hnut√≠ch.
    """
    name = "reddit_spider"
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.config_loader = get_config_loader()
        self.source_config = self.config_loader.get_source('reddit')
        
        if not self.source_config or self.source_config.get('type') != 'social_api':
            raise ValueError("Reddit zdroj nen√≠ nakonfigurov√°n nebo nen√≠ typu social_api")
        
        # Naƒçti API kl√≠ƒçe z environment nebo config
        client_id = os.getenv('REDDIT_CLIENT_ID') or self.source_config.get('auth', {}).get('client_id')
        client_secret = os.getenv('REDDIT_CLIENT_SECRET') or self.source_config.get('auth', {}).get('client_secret')
        user_agent = os.getenv('REDDIT_USER_AGENT') or self.source_config.get('auth', {}).get('user_agent')
        
        if not all([client_id, client_secret, user_agent]):
            self.logger.warning("‚ö†Ô∏è Reddit API kl√≠ƒçe nejsou nastaveny. Nastavte env promƒõnn√©:")
            self.logger.warning("   REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT")
            raise ValueError("Chybƒõj√≠ Reddit API kl√≠ƒçe")
        
        # Inicializuj Reddit API
        self.reddit = praw.Reddit(
            client_id=client_id,
            client_secret=client_secret,
            user_agent=user_agent
        )
        
        self.logger.info("‚úÖ Reddit API inicializov√°n")
    
    def start_requests(self):
        """Pro Reddit pou≈æ√≠v√°me p≈ô√≠m√© API m√≠sto HTTP request≈Ø."""
        # V Scrapymu mus√≠me vr√°tit alespo≈à jeden request
        yield scrapy.Request(
            'https://www.reddit.com/r/occult/.json',
            callback=self.parse_reddit,
            dont_download=True,
            dont_filter=True
        )
    
    def parse_reddit(self, response):
        """Extrahuje p≈ô√≠spƒõvky z Redditu a filtruje relevantn√≠ obsah."""
        try:
            self.logger.info("üì± Hled√°m p≈ô√≠spƒõvky na Redditu...")
            
            subreddits = self.source_config.get('subreddits', [])
            search_terms = self.source_config.get('search_terms', [])
            output_csv = self.source_config.get('output_csv', 'export/csv/reddit_raw.csv')
            
            submissions = []
            
            # Hledej v konkr√©tn√≠ch subredditech
            for subreddit_name in subreddits:
                subreddit_name = subreddit_name.replace('r/', '')
                try:
                    subreddit = self.reddit.subreddit(subreddit_name)
                    
                    # Hledej nov√© p≈ô√≠spƒõvky
                    for submission in subreddit.new(limit=50):
                        combined_text = f"{submission.title} {submission.selftext}"
                        
                        # Kontrola relevance
                        if contains_relevant_keywords(combined_text):
                            submissions.append({
                                'source_name': 'Reddit',
                                'source_type': 'Social Media',
                                'title': submission.title,
                                'url': f"https://reddit.com{submission.permalink}",
                                'text': submission.selftext[:5000],  # Omezen√≠ d√©lky
                                'scraped_at': datetime.utcnow().isoformat(),
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created': datetime.fromtimestamp(submission.created_utc).isoformat(),
                                'subreddit': subreddit_name
                            })
                            self.logger.info(f"‚úì Reddit: {submission.title[:50]}")
                
                except Exception as e:
                    self.logger.error(f"‚ùå Chyba p≈ôi hled√°n√≠ v r/{subreddit_name}: {e}")
                    continue
            
            # Tak√© hledej podle kl√≠ƒçov√Ωch slov
            for term in search_terms:
                try:
                    for submission in self.reddit.subreddit('all').search(term, time_filter='month', limit=30):
                        combined_text = f"{submission.title} {submission.selftext}"
                        
                        if contains_relevant_keywords(combined_text):
                            submissions.append({
                                'source_name': 'Reddit',
                                'source_type': 'Social Media',
                                'title': submission.title,
                                'url': f"https://reddit.com{submission.permalink}",
                                'text': submission.selftext[:5000],
                                'scraped_at': datetime.utcnow().isoformat(),
                                'author': str(submission.author),
                                'score': submission.score,
                                'num_comments': submission.num_comments,
                                'created': datetime.fromtimestamp(submission.created_utc).isoformat(),
                                'search_term': term
                            })
                            self.logger.info(f"‚úì Reddit (hled√°n√≠ '{term}'): {submission.title[:50]}")
                
                except Exception as e:
                    self.logger.error(f"‚ùå Chyba p≈ôi hled√°n√≠ '{term}': {e}")
                    continue
            
            self.logger.info(f"üìä Nalezeno {len(submissions)} relevantn√≠ch p≈ô√≠spƒõvk≈Ø na Redditu")
            
            for submission in submissions:
                yield submission
        
        except Exception as e:
            self.logger.error(f"‚ùå Chyba p≈ôi parsov√°n√≠ Redditu: {e}")


class XTwitterSpider(scrapy.Spider):
    """
    Spider pro X (Twitter) API v2.
    Hled√° tweety o sekt√°ch a n√°bo≈æensk√Ωch hnut√≠ch.
    """
    name = "x_twitter_spider"
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.config_loader = get_config_loader()
        self.source_config = self.config_loader.get_source('x_twitter')
        
        if not self.source_config or self.source_config.get('type') != 'social_api':
            raise ValueError("X/Twitter zdroj nen√≠ nakonfigurov√°n nebo nen√≠ typu social_api")
        
        # Naƒçti bearer token
        self.bearer_token = os.getenv('X_BEARER_TOKEN') or self.source_config.get('auth', {}).get('bearer_token')
        
        if not self.bearer_token:
            self.logger.warning("‚ö†Ô∏è X/Twitter API token nen√≠ nastaven. Nastavte env promƒõnnou:")
            self.logger.warning("   X_BEARER_TOKEN")
            raise ValueError("Chyb√≠ X/Twitter API token")
        
        self.base_url = self.source_config.get('url', 'https://api.twitter.com/2')
        self.logger.info("‚úÖ X/Twitter API inicializov√°n")
    
    def start_requests(self):
        """Generuje po≈æadavky pro X API."""
        search_queries = self.source_config.get('search_queries', [])
        
        for query in search_queries:
            search_url = f"{self.base_url}/tweets/search/recent"
            
            params = {
                'query': query,
                **self.source_config.get('api_params', {})
            }
            
            headers = self._get_headers()
            
            yield scrapy.Request(
                search_url,
                method='GET',
                meta={'query': query},
                headers=headers,
                callback=self.parse_x,
                dont_filter=True,
                errback=self.handle_error
            )
    
    def _get_headers(self):
        """Vr√°t√≠ headers s bearer token."""
        return {
            'Authorization': f'Bearer {self.bearer_token}',
            'User-Agent': 'ProjectInfinit/1.0'
        }
    
    def parse_x(self, response):
        """Parsuje odpovƒõƒè z X API a extrahuje tweety."""
        try:
            import json
            data = json.loads(response.text)
            
            query = response.meta.get('query')
            self.logger.info(f"üì± Zpracov√°v√°m tweety pro dotaz: '{query}'")
            
            tweets = data.get('data', [])
            includes = data.get('includes', {})
            users = {user['id']: user['username'] for user in includes.get('users', [])}
            
            for tweet in tweets:
                text = tweet.get('text', '')
                
                # Kontrola relevance
                if contains_relevant_keywords(text):
                    author_id = tweet.get('author_id', '')
                    author_name = users.get(author_id, 'Unknown')
                    
                    yield {
                        'source_name': 'X (Twitter)',
                        'source_type': 'Social Media',
                        'title': text[:100],
                        'url': f"https://twitter.com/i/web/status/{tweet['id']}",
                        'text': text,
                        'scraped_at': datetime.utcnow().isoformat(),
                        'author': author_name,
                        'created': tweet.get('created_at', ''),
                        'metrics': tweet.get('public_metrics', {}),
                        'search_query': query
                    }
                    self.logger.info(f"‚úì X: @{author_name}: {text[:50]}")
            
            self.logger.info(f"üìä Nalezeno {len([t for t in tweets if contains_relevant_keywords(t.get('text', ''))])} relevantn√≠ch tweet≈Ø")
        
        except Exception as e:
            self.logger.error(f"‚ùå Chyba p≈ôi parsov√°n√≠ X API: {e}")
    
    def handle_error(self, failure):
        """Zpracov√°n√≠ chyb API."""
        self.logger.error(f"‚ùå X API chyba: {failure.value}")
        query = failure.request.meta.get('query', 'Unknown')
        self.logger.error(f"   Dotaz: {query}")
