# 游늬 scraping/info_dingir_spider.py
import scrapy
from datetime import datetime
from .spider_settings import ETHICAL_SCRAPING_SETTINGS, CSV_EXPORT_SETTINGS
from .keywords import contains_relevant_keywords

class InfoDingirSpider(scrapy.Spider):
    name = "info_dingir"
    allowed_domains = ["info.dingir.cz"]
    start_urls = ["https://info.dingir.cz/"]

    # Spojen칤 z치kladn칤ho etick칠ho nastaven칤 s nastaven칤m pro tento spider
    custom_settings = {
        **ETHICAL_SCRAPING_SETTINGS,
        "FEEDS": {
            "export/csv/info_dingir_raw.csv": CSV_EXPORT_SETTINGS
        },
        "LOG_LEVEL": "INFO"
    }

    def start_requests(self):
        """P콏id치v치 po캜치te캜n칤 request s informac칤 o zdroji v meta"""
        for url in self.start_urls:
            yield scrapy.Request(
                url,
                callback=self.parse,
                meta={
                    'source_name': 'Dingir.cz',
                    'source_type': 'Odborn칳 web'
                }
            )

    def parse(self, response):
        """Parsov치n칤 seznamu 캜l치nk콢 s filtrov치n칤m podle kl칤캜ov칳ch slov"""
        articles = response.css("article")
        for article in articles:
            title = article.css("h2.entry-title a::text").get()
            url = article.css("h2.entry-title a::attr(href)").get()
            excerpt = article.css(".entry-summary p::text").get()
            
            # Kontrola relevance podle nadpisu a 칰ryvku
            if contains_relevant_keywords(f"{title} {excerpt or ''}"):
                meta = {
                    **response.meta,
                    'title': title,
                    'excerpt': excerpt
                }
                yield response.follow(
                    url,
                    callback=self.parse_article,
                    meta=meta
                )

        # Str치nkov치n칤 s respektov치n칤m robots.txt
        next_page = response.css("a.next::attr(href)").get()
        if next_page:
            yield response.follow(
                next_page,
                callback=self.parse,
                meta=response.meta
            )

    def parse_article(self, response):
        """Parsov치n칤 캜l치nku s validac칤 dat a kontrolou relevance."""
        try:
            # Extrakce z치kladn칤ch dat
            title = response.meta.get('title') or response.css("h1.entry-title::text").get()
            paragraphs = response.css("div.entry-content p::text").getall()
            full_text = " ".join(p.strip() for p in paragraphs if p.strip())
            date_str = response.css("time.entry-date::attr(datetime)").get()
            author = response.css(".author-name::text, .byline::text").get() or "Nezn치m칳"
            tags = response.css(".tags-links a::text").getall()

            # Validace povinn칳ch pol칤 a relevance
            if not all([title, full_text, response.url]):
                self.logger.warning(f"Chyb칤 povinn치 data v 캜l치nku: {response.url}")
                return

            # Kontrola relevance cel칠ho textu
            if not contains_relevant_keywords(full_text):
                self.logger.info(f"캛l치nek nen칤 relevantn칤: {response.url}")
                return

            # Zpracov치n칤 data s fallbackem
            try:
                published_at = datetime.strptime(date_str, "%Y-%m-%dT%H:%M:%S%z") if date_str else None
            except ValueError:
                published_at = None

            # V칳stup ve form치tu kompatibiln칤m s DB sch칠matem
            yield {
                'source_name': response.meta.get('source_name', 'Dingir.cz'),
                'source_type': response.meta.get('source_type', 'Odborn칳 web'),
                'title': title.strip(),
                'url': response.url,
                'text': full_text,
                'scraped_at': datetime.utcnow().isoformat(),
                'published_at': published_at.isoformat() if published_at else None,
                'author': author.strip(),
                'tags': tags,
                'excerpt': response.meta.get('excerpt', '')
            }
            except ValueError:
                pass

        yield {
            "source_name": "info.dingir.cz",
            "source_type": "web",
            "title": title,
            "url": response.url,
            "text": full_text,
            "publication_date": date.isoformat() if date else None,
            "scraped_at": datetime.datetime.now().isoformat()
        }