# 游늬 scraping/wikipedia_spider.py
# Wikipedia API m칤sto p콏칤m칠ho scrapingu - eti캜t캩j코칤 p콏칤stup s filtrov치n칤m

import scrapy
import json
import re
from datetime import datetime
from urllib.parse import urlencode, unquote
from .spider_settings import ETHICAL_SCRAPING_SETTINGS, CSV_EXPORT_SETTINGS
from .keywords import contains_relevant_keywords, KNOWN_MOVEMENTS, YEAR_PATTERNS

class WikipediaSpider(scrapy.Spider):
    name = "wikipedia"
    allowed_domains = ["cs.wikipedia.org"]
    api_url = "https://cs.wikipedia.org/w/api.php"
    
    # Kombinace etick칠ho nastaven칤 s vlastn칤m nastaven칤m pro API
    custom_settings = {
        **ETHICAL_SCRAPING_SETTINGS,
        "ROBOTSTXT_OBEY": True,
        "DOWNLOAD_DELAY": 3,  # Vy코코칤 zpo쬯캩n칤 pro Wikipedia API
        "CONCURRENT_REQUESTS_PER_DOMAIN": 1,
        "FEEDS": {
            "export/csv/wikipedia_raw.csv": CSV_EXPORT_SETTINGS
        },
        "LOG_LEVEL": "INFO"
    }

    def start_requests(self):
        """Inicializace API requestu s meta daty"""
        params = {
            "action": "parse",
            "page": "Seznam_c칤rkv칤_a_n치bo쬰nsk칳ch_spole캜nost칤_v_캛esku",
            "format": "json",
            "prop": "wikitext",
            "formatversion": 2
        }
        
        url = f"{self.api_url}?{urlencode(params)}"
        yield scrapy.Request(
            url=url,
            callback=self.parse_api,
            meta={
                'source_name': 'Wikipedia',
                'source_type': 'Encyklopedie',
                'base_url': 'https://cs.wikipedia.org/wiki/'
            },
            errback=self.handle_error
        )

    def parse_api(self, response):
        """Parsov치n칤 API odpov캩di a extrakce seznamu n치bo쬰nsk칳ch skupin."""
        try:
            data = json.loads(response.text)
            if 'parse' in data and 'wikitext' in data['parse']:
                wikitext = data['parse']['wikitext']
                
                # Hled치me n치zvy skupin v wikitextu
                movement_matches = re.finditer(r"\[\[(.*?)\]\]", wikitext)
                for match in movement_matches:
                    page_name = match.group(1).split('|')[0]  # Bere prvn칤 캜치st p콏ed |
                    if any(mov.lower() in page_name.lower() for mov in KNOWN_MOVEMENTS):
                        params = {
                            "action": "query",
                            "titles": page_name,
                            "prop": "extracts|categories|info",
                            "exintro": True,
                            "explaintext": True,
                            "format": "json",
                            "formatversion": 2,
                            "inprop": "url"
                        }
                        api_url = f"{self.api_url}?{urlencode(params)}"
                        yield scrapy.Request(
                            url=api_url,
                            callback=self.parse_movement,
                            meta={
                                **response.meta,
                                'movement_name': page_name
                            },
                            errback=self.handle_error
                        )
        except Exception as e:
            self.logger.error(f"Chyba p콏i parsov치n칤 API odpov캩di: {e}")
            
    def parse_movement(self, response):
        """Zpracov치n칤 detail콢 o n치bo쬰nsk칠m hnut칤."""
        try:
            data = json.loads(response.text)
            pages = data.get('query', {}).get('pages', [])
            if not pages:
                return
                
            page = pages[0]  # Ve formatversion 2 dost치v치me pole
            if 'missing' in page:
                return
                
            title = page.get('title', '')
            text = page.get('extract', '')
            categories = [cat.get('title', '') for cat in page.get('categories', [])]
            
            # Extrakce roku zalo쬰n칤
            year_founded = None
            for pattern in YEAR_PATTERNS:
                match = re.search(pattern, text)
                if match:
                    year_founded = match.group(1)
                    break
                    
            # Kontrola relevance
            if not contains_relevant_keywords(text):
                return
                
            yield {
                'source_name': response.meta.get('source_name', 'Wikipedia'),
                'source_type': response.meta.get('source_type', 'Encyklopedie'),
                'title': title,
                'url': page.get('canonicalurl', ''),
                'text': text,
                'scraped_at': datetime.utcnow().isoformat(),
                'categories': categories,
                'year_founded': year_founded,
                'movement_name': response.meta.get('movement_name'),
                'last_modified': page.get('touched')
            }
        except Exception as e:
            self.logger.error(f"Chyba p콏i parsov치n칤 detail콢 hnut칤: {e}")
            
    def handle_error(self, failure):
        """Roz코칤콏en칠 logov치n칤 chyb API po쬬davk콢."""
        request = failure.request
        self.logger.error(f"Chyba API po쬬davku na {request.url}: {failure.value}")
        self.logger.error(f"Headers: {request.headers}")
        self.logger.error(f"Meta: {request.meta}")
        if hasattr(failure.value, 'response') and failure.value.response:
            self.logger.error(f"Response body: {failure.value.response.body}")
        try:
            data = json.loads(response.text)
            wikitext = data.get("parse", {}).get("wikitext", "")
            
            if not wikitext:
                self.logger.error("Pr치zdn치 odpov캩캞 z API")
                return

            # Extrakce n치zv콢 organizac칤
            lines = wikitext.split("\n")
            for line in lines:
                if line.startswith("*"):
                    name = line.lstrip("* ").strip()
                    if name:
                        # Generov치n칤 캜ist칠ho v칳stupu
                        yield {
                            "source_name": response.meta['source_name'],
                            "source_type": response.meta['source_type'],
                            "title": name,
                            "url": response.meta['base_url'] + "Seznam_c칤rkv칤_a_n치bo쬰nsk칳ch_spole캜nost칤_v_캛esku",
                            "text": f"Registrovan치 c칤rkev/n치bo쬰nsk치 spole캜nost: {name}",
                            "scraped_at": datetime.utcnow().isoformat()
                        }
        except json.JSONDecodeError as e:
            self.logger.error(f"Chyba p콏i parsov치n칤 JSON: {e}")
        except Exception as e:
            self.logger.error(f"Neo캜ek치van치 chyba: {e}")

    def handle_error(self, failure):
        """Zpracov치n칤 chyb p콏i API requestech"""
        self.logger.error(f"Chyba p콏i API requestu: {failure.value}")
                    }
