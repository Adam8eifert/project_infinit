# ğŸ“˜ Project Infinit - Analysis of New Religious Movements in the Czech Republic

An ETL pipeline for collecting, analyzing, and visualizing information about new religious movements in the Czech Republic. Features ethical web scraping, NLP analysis, and structured data storage.

[ğŸ‡¨ğŸ‡¿ ÄŒeskÃ¡ verze nÃ­Å¾e](#-projekt-infinit---analÃ½za-novÃ½ch-nÃ¡boÅ¾enskÃ½ch-hnutÃ­-v-Är)

## ğŸŒŸ Features

- Ethical web scraping with rate limiting and robots.txt respect
- Automated data collection from multiple sources:
  - RSS feeds from specialized websites
  - REST APIs (Wikipedia, SOCCAS)
  - Social media APIs (Reddit, X/Twitter)
  - Web scraping for news aggregators
- Natural Language Processing:
  - **Multilingual support**: Czech (Stanza) and English (Stanza) with automatic language detection
  - Named Entity Recognition via Hugging Face Transformers (WikiNeural, BioBERT)
  - Sentiment analysis via multilingual BERT models (nlptown/bert-base-multilingual)
  - Movement classification and relationship analysis
  - Academic document processing (PDF, DOC, DOCX text extraction)
- Structured data storage in PostgreSQL/SQLite
- Export capabilities for further analysis and Power BI integration
- Comprehensive testing suite with pytest
- Type-safe code with Pylance/Pyright integration

## ğŸ”§ Technology Stack

- **Python 3.10+** - Core programming language
- **Scrapy 2.13+** - Web scraping framework with custom settings
- **Stanza 1.11+** - Czech and English NLP (tokenization, POS, lemma, NER)
- **langdetect** - Automatic language detection (Czech/English)
- **Hugging Face Transformers** - Advanced NLP models for NER and multilingual sentiment
- **SQLAlchemy 2.0+** - Database ORM with PostgreSQL/SQLite support
- **PRAW 7.8+** - Reddit API client
- **Tweepy 4.14+** - X (Twitter) API client
- **PyMuPDF 1.23+** - PDF text extraction
- **python-docx 1.1+** - Word document (.doc/.docx) processing
- **pandas 2.3+** - Data manipulation and CSV processing
- **pytest** - Testing framework with mocking
- **PyYAML** - Configuration management

## ğŸ—‚ï¸ Project Structure

```text
project_infinit/
â”œâ”€â”€ extracting/              # Web scrapers and configurations
â”‚   â”œâ”€â”€ sources_config.yaml    # Centralized source configuration
â”‚   â”œâ”€â”€ spider_settings.py     # Ethical scraping settings
â”‚   â”œâ”€â”€ keywords.py           # Keyword filtering utilities
â”‚   â”œâ”€â”€ config_loader.py      # YAML configuration loader
â”‚   â”œâ”€â”€ rss_spider.py        # Universal RSS feed scraper
â”‚   â”œâ”€â”€ api_spider.py        # Universal API scraper
â”‚   â”œâ”€â”€ social_media_spider.py # Social media API scraper
â”‚   â”œâ”€â”€ google_spider.py     # Google News scraper
â”‚   â”œâ”€â”€ medium_seznam_spider.py # Medium/Seznam scraper
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ processing/            # Data processing and analysis
â”‚   â”œâ”€â”€ nlp_analysis.py       # NLP pipeline with Czech support
â”‚   â”œâ”€â”€ import_csv_to_db.py   # CSV database ingestion utilities
â”‚   â”œâ”€â”€ import_pdf_to_db.py   # PDF processing and ingestion
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ database/              # Database layer
â”‚   â”œâ”€â”€ db_loader.py          # SQLAlchemy models and connections
â”‚   â”œâ”€â”€ models/              # Database models
â”‚   â”‚   â”œâ”€â”€ source.py        # Source model
â”‚   â”‚   â”œâ”€â”€ movement.py      # Movement model
â”‚   â”‚   â”œâ”€â”€ alias.py         # Alias model
â”‚   â”‚   â”œâ”€â”€ location.py      # Location model
â”‚   â”‚   â”œâ”€â”€ source_quality.py # Source quality model
â”‚   â”‚   â”œâ”€â”€ geographic_analysis.py # Geographic analysis model
â”‚   â”‚   â”œâ”€â”€ temporal_analysis.py # Temporal analysis model
â”‚   â”‚   â””â”€â”€ __init__.py      # Models init
â”‚   â”œâ”€â”€ schema.sql           # Database schema
â”‚   â”œâ”€â”€ views.sql            # Database views
â”‚   â”œâ”€â”€ deduplicate_sources.py # Source deduplication utilities
â”‚   â”œâ”€â”€ ANALYTICS_README.md  # Analytics documentation
â”‚   â”œâ”€â”€ migrate_analytics.py # Analytics migration script
â”‚   â”œâ”€â”€ migrations/          # Database migrations
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ testing/               # Test suite
â”‚   â”œâ”€â”€ test_*.py           # Unit tests for all modules
â”‚   â”œâ”€â”€ README.md           # Testing documentation
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ export/                # Output files and exports
â”‚   â”œâ”€â”€ csv/               # Scraped and processed CSV data
â”‚   â””â”€â”€ to_powerbi.py      # Power BI export utilities
â”œâ”€â”€ data/                  # Input data directory
â”œâ”€â”€ academic_data/         # Academic documents (PDF, DOC, DOCX)
â”œâ”€â”€ nnh-db/                # Docker database setup
â”‚   â”œâ”€â”€ docker             # Docker files
â”‚   â””â”€â”€ docker-compose.yml # Docker Compose configuration
â”œâ”€â”€ .github/               # GitHub configuration
â”‚   â””â”€â”€ copilot-instructions.md # AI assistant instructions
â”œâ”€â”€ config.py              # Database and app configuration
â”œâ”€â”€ main.py                # Main ETL orchestrator
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ environment.yml        # Conda environment
â”œâ”€â”€ pyrightconfig.json    # Pyright type checking config
â”œâ”€â”€ LICENSE                # Project license
â”œâ”€â”€ SOCIAL_MEDIA_SETUP.md  # Social media API setup guide
â”œâ”€â”€ import_log.txt         # CSV import log
â”œâ”€â”€ pdf_import_log.txt     # PDF import log
â””â”€â”€ readme.md             # This file
```

## ğŸš€ Quick Start

### 1. Clone and Setup Environment

```bash
git clone https://github.com/Adam8eifert/project_infinit.git
cd project_infinit

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/macOS
# or
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt
```

### 2. Setup NLP Models and Dependencies

```bash
# Using Mamba (recommended) for better dependency resolution
mamba create -n project_infinit -y --file environment.yml
mamba activate project_infinit

# Or using pip (fallback)
pip install -r requirements.txt

# Stanza will auto-download Czech and English models on first use
```

### 3. Configure Database

The project supports both PostgreSQL and SQLite.

```python
# config.py (default configuration)
DB_URI = "sqlite:///data/project_infinit.db"
```

For PostgreSQL:

```python
DB_URI = "postgresql+psycopg2://username:password@localhost/nsm_db"
```

### 4. Configure Social Media APIs (Optional)

To enable Reddit and X (Twitter) data collection:

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your API keys (see .env.example for instructions)
```

### 5. Run the Pipeline

```bash
# Run complete ETL pipeline
python main.py

# Or run individual components
scrapy runspider extracting/rss_spider.py
scrapy runspider extracting/api_spider.py
scrapy runspider extracting/social_media_spider.py
```

## ğŸ”„ ETL Pipeline Steps

1. **Data Collection** (`run_spiders()`)
   - RSS feeds from 12+ specialized and mainstream sources
   - REST APIs (Wikipedia, SOCCAS Encyclopedia)
   - Social media posts from Reddit and X/Twitter
   - Web scraping for news aggregators (Google News, Medium, Seznam)
   - Academic documents (PDF, DOC, DOCX from `academic_data/`)

2. **CSV Import** (`process_csv()`)
   - Load scraped CSV files from `export/csv/`
   - Deduplicate by URL and content hash (SHA256)
   - Import validated sources to database

3. **Academic Document Processing** (`process_academic_documents()`)
   - Extract text from PDF (PyMuPDF), DOC/DOCX (python-docx)
   - Convert legacy .doc to .docx via LibreOffice
   - Validate content (min 50 words + religious keywords)
   - Match documents to known movements
   - Import to database with metadata

4. **Entity Extraction** (`process_entities()`)
   - NER: Extract persons, organizations, locations from content
   - Create Alias records for movement name variations
   - Create Location records for geographic references
   - Match entities to movement database

5. **NLP Analysis** (`run_nlp()`)
   - **Language detection**: Automatically identify Czech/English
   - **Sentiment analysis**: Score emotional tone (positive/negative/neutral)
   - **Lemmatization & POS tagging**: Via Stanza pipelines
   - Generate sentiment logs and analysis reports

6. **Data Storage & Export** (`load_scraped_csvs()`)
   - Persist all processed data to PostgreSQL/SQLite
   - Generate CSV exports for Power BI
   - Create comprehensive audit logs

## ğŸ“Š Data Sources

The pipeline collects data from multiple sources configured in `extracting/sources_config.yaml`:

| Type       | Source                      | Method        | Status        | Description                  |
| ---------- | --------------------------- | ------------- | ------------- | ---------------------------- |
| RSS        | Sekty.TV                    | Feed Parser   | âœ… Active     | Specialized sect information |
| RSS        | Sekty.cz                    | Feed Parser   | âœ… Active     | Religious movement news      |
| RSS        | Dingir.cz                   | Feed Parser   | âœ… Active     | Academic religious studies   |
| RSS        | PastorÃ¡lnÃ­ pÃ©Äe             | Feed Parser   | âœ… Active     | Pastoral care resources      |
| RSS        | Seznam ZprÃ¡vy               | Feed Parser   | âœ… Active     | Czech news portal            |
| RSS        | ÄŒeskÃ½ rozhlas (iRozhlas.cz) | Feed Parser   | âœ… Active     | Public radio news            |
| RSS        | AktuÃ¡lnÄ›.cz                 | Feed Parser   | âœ… Active     | Czech news website           |
| RSS        | Forum24.cz                  | Feed Parser   | âœ… Active     | Discussion forum             |
| RSS        | DenÃ­k Alarm                 | Feed Parser   | âœ… Active     | Investigative journalism     |
| RSS        | Blesk.cz                    | Feed Parser   | âœ… Active     | Tabloid news                 |
| Web        | Medium.seznam.cz            | Scrapy        | âœ… Active     | Blog articles                |
| API        | SociologickÃ½ Ãºstav AVÄŒR     | MediaWiki API | âœ… Active     | Academic research database   |
| API        | Wikipedia (Czech)           | MediaWiki API | âœ… Active     | Encyclopedia articles        |
| Search API | Google News                 | Custom API    | â¸ï¸ Legacy     | News aggregation             |
| Social API | Reddit                      | PRAW          | âœ… Configured | Community discussions        |
| Social API | X (Twitter)                 | Tweepy        | âœ… Configured | Social media posts           |

## ğŸ§ª Testing

Run the comprehensive test suite:

```bash
# Install test dependencies
pip install pytest pytest-mock

# Run all tests
pytest testing/

# Run specific test
pytest testing/test_nlp_analysis.py -v
```

## ğŸ“¦ Dependencies

### Core Requirements

- Python 3.10+
- Scrapy 2.13+ (web scraping)
- SQLAlchemy 2.0+ (database ORM)
- Stanza 1.11+ (Czech/English NLP)
- langdetect (automatic language detection)
- transformers 4.52+ (BERT sentiment, multilingual models)
- pandas 2.3+ (data manipulation)

### API Clients

- praw 7.8+ (Reddit)
- tweepy 4.14+ (X/Twitter)
- requests 2.31+
- feedparser 6.0+

### Data Processing & NLP

- PyMuPDF 1.23+ (PDF text extraction)
- python-docx 1.1+ (Word documents: .doc, .docx; auto-conversion .doc â†’ .docx)
- openpyxl (Excel spreadsheet handling)
- fuzzywuzzy 0.18+ (fuzzy text matching for entity resolution)
- python-dotenv 1.0+ (environment variable management)
- numpy (numerical operations for NLP)

### Development

- pytest (testing)
- pyright (type checking)

## ğŸ“Š Outputs

- **Database**: Structured data in PostgreSQL/SQLite with relationships
- **CSV Exports**: Processed data in `export/csv/` directory
- **Power BI**: Direct integration via `export/to_powerbi.py`
- **Analysis Reports**: NLP insights and entity relationships
- **Logs**: Comprehensive logging in `import_log.txt`

## ğŸ›¡ï¸ Ethical Guidelines

- âœ… Respect robots.txt and rate limits
- âœ… Proper user agent identification
- âœ… Data minimization and privacy protection
- âœ… Source attribution and transparency
- âœ… Academic and research-focused data collection
- âœ… No personal data collection without consent

## ğŸ”§ Development

### Code Quality

- Type hints throughout codebase
- Pylance/Pyright type checking
- Comprehensive test coverage
- Ethical scraping practices
- Modular architecture

### Adding New Sources

1. Add configuration to `extracting/sources_config.yaml`
2. Implement spider in `extracting/` directory
3. Add tests in `testing/` directory
4. Update main.py orchestration

### Database Schema

The database uses SQLAlchemy ORM with the following main entities:

- **Source**: Articles, posts, and documents
- **Movement**: Religious movements and sects
- **Alias**: Alternative names for movements
- **Location**: Geographic references

## ğŸ“¬ Future Development

- [ ] Enhanced NLP models for Czech language
- [ ] Real-time social media monitoring
- [ ] Advanced entity relationship analysis
- [ ] Geographic visualization of movements
- [ ] Trend analysis and time series
- [ ] REST API for data access
- [ ] Web dashboard interface
- [ ] Multi-language support expansion

---

## ğŸ‡¨ğŸ‡¿ Projekt Infinit - AnalÃ½za novÃ½ch nÃ¡boÅ¾enskÃ½ch hnutÃ­ v ÄŒR

ETL pipeline pro sbÄ›r, analÃ½zu a vizualizaci informacÃ­ o novÃ½ch nÃ¡boÅ¾enskÃ½ch hnutÃ­ch v ÄŒeskÃ© republice. Zahrnuje etickÃ½ web scraping, NLP analÃ½zu a strukturovanÃ© uklÃ¡dÃ¡nÃ­ dat.

## ğŸŒŸ Funkce

- EtickÃ½ web scraping s omezenÃ­m rychlosti a respektovÃ¡nÃ­m robots.txt
- AutomatizovanÃ½ sbÄ›r dat z vÃ­ce zdrojÅ¯:
  - RSS feedy ze specializovanÃ½ch webÅ¯
  - REST API (Wikipedia, SOCCAS)
  - SociÃ¡lnÃ­ mÃ©dia API (Reddit, X/Twitter)
  - Web scraping pro news agregÃ¡tory
- ZpracovÃ¡nÃ­ pÅ™irozenÃ©ho jazyka:
  - Podpora ÄeÅ¡tiny pÅ™es spaCy
  - RozpoznÃ¡vÃ¡nÃ­ entit pÅ™es Hugging Face Transformers
  - AnalÃ½za sentimentu pÅ™es multijazyÄnÃ© BERT modely
  - Klasifikace hnutÃ­ a analÃ½za vztahÅ¯
- StrukturovanÃ© uklÃ¡dÃ¡nÃ­ dat v PostgreSQL/SQLite
- ExportnÃ­ moÅ¾nosti pro dalÅ¡Ã­ analÃ½zu a Power BI integraci
- KomplexnÃ­ testovacÃ­ sada s pytest
- Type-safe kÃ³d s Pylance/Pyright integracÃ­

## ğŸ”§ TechnologickÃ½ stack

- **Python 3.10+** - ZÃ¡kladnÃ­ programovacÃ­ jazyk
- **Scrapy** - Framework pro web scraping s vlastnÃ­mi nastavenÃ­mi
- **spaCy** - NLP toolkit pro zpracovÃ¡nÃ­ ÄeÅ¡tiny
- **Hugging Face Transformers** - PokroÄilÃ© NLP modely pro NER a sentiment
- **SQLAlchemy** - Database ORM s podporou PostgreSQL/SQLite
- **PRAW** - Reddit API klient
- **Tweepy** - X (Twitter) API klient
- **pandas** - Manipulace s daty a CSV zpracovÃ¡nÃ­
- **pytest** - TestovacÃ­ framework s mocking
- **PyYAML** - SprÃ¡va konfigurace

## ğŸ—‚ï¸ Struktura projektu

```text
project_infinit/
â”œâ”€â”€ extracting/              # Web scrapery a konfigurace
â”‚   â”œâ”€â”€ sources_config.yaml    # CentralizovanÃ¡ konfigurace zdrojÅ¯
â”‚   â”œâ”€â”€ spider_settings.py     # EtickÃ¡ scraping nastavenÃ­
â”‚   â”œâ”€â”€ keywords.py           # Utility pro filtrovÃ¡nÃ­ klÃ­ÄovÃ½ch slov
â”‚   â”œâ”€â”€ config_loader.py      # NaÄÃ­tÃ¡nÃ­ YAML konfigurace
â”‚   â”œâ”€â”€ rss_spider.py        # UnivezÃ¡lnÃ­ RSS feed scraper
â”‚   â”œâ”€â”€ api_spider.py        # UnivezÃ¡lnÃ­ API scraper
â”‚   â”œâ”€â”€ social_media_spider.py # Social media API scraper
â”‚   â”œâ”€â”€ google_spider.py     # Google News scraper
â”‚   â”œâ”€â”€ medium_seznam_spider.py # Medium/Seznam scraper
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ processing/            # ZpracovÃ¡nÃ­ a analÃ½za dat
â”‚   â”œâ”€â”€ nlp_analysis.py       # NLP pipeline s podporou ÄeÅ¡tiny
â”‚   â””â”€â”€ import_csv_to_db.py   # Utility pro import do databÃ¡ze
â”œâ”€â”€ database/              # DatabÃ¡zovÃ¡ vrstva
â”‚   â”œâ”€â”€ db_loader.py          # SQLAlchemy modely a pÅ™ipojenÃ­
â”‚   â”œâ”€â”€ models/              # DatabÃ¡zovÃ© modely
â”‚   â”‚   â”œâ”€â”€ source.py        # Model zdrojÅ¯
â”‚   â”‚   â”œâ”€â”€ movement.py      # Model hnutÃ­
â”‚   â”‚   â”œâ”€â”€ alias.py         # Model aliasÅ¯
â”‚   â”‚   â””â”€â”€ location.py      # Model lokacÃ­
â”‚   â””â”€â”€ schema.sql           # DatabÃ¡zovÃ© schÃ©ma
â”œâ”€â”€ testing/               # TestovacÃ­ sada
â”‚   â”œâ”€â”€ test_*.py           # Unit testy pro vÅ¡echny moduly
â”‚   â””â”€â”€ README.md           # Dokumentace testovÃ¡nÃ­
â”œâ”€â”€ export/                # VÃ½stupnÃ­ soubory a exporty
â”‚   â”œâ”€â”€ csv/               # Scraped a zpracovanÃ¡ CSV data
â”‚   â””â”€â”€ to_powerbi.py      # Utility pro Power BI export
â”œâ”€â”€ data/                  # VstupnÃ­ data
â”‚   â”œâ”€â”€ pdf/               # PDF dokumenty pro zpracovÃ¡nÃ­
â”‚   â””â”€â”€ xlsx/              # Excel soubory pro konverzi
â”œâ”€â”€ dags/                  # Apache Airflow DAGs (volitelnÃ©)
â”œâ”€â”€ .github/               # GitHub konfigurace
â”œâ”€â”€ config.py              # Konfigurace databÃ¡ze a aplikace
â”œâ”€â”€ main.py                # HlavnÃ­ ETL orchestrÃ¡tor
â”œâ”€â”€ requirements.txt       # Python zÃ¡vislosti
â”œâ”€â”€ environment.yml        # Conda prostÅ™edÃ­
â”œâ”€â”€ pyrightconfig.json    # Konfigurace Pyright type checking
â””â”€â”€ readme.md             # Tento soubor
```

## ğŸ‡¨ğŸ‡¿ RychlÃ½ start (CZ)

### 1. KlonovÃ¡nÃ­ a pÅ™Ã­prava prostÅ™edÃ­

```bash
git clone https://github.com/adamseifert/project_infinit.git
cd project_infinit

# VytvoÅ™enÃ­ Mamba environment (doporuÄeno)
mamba create -n project_infinit -y --file environment.yml
mamba activate project_infinit

# Nebo pip (fallback)
pip install -r requirements.txt
```

### 2. NastavenÃ­ NLP a zÃ¡vislostÃ­

Stanza automaticky stÃ¡hne ÄeskÃ© a anglickÃ© modely pÅ™i prvnÃ­m spuÅ¡tÄ›nÃ­:

```bash
# Å½Ã¡dnÃ© dalÅ¡Ã­ kroky nejsou nutnÃ© â€“ NLP se inicializuje pÅ™i main.py
```

### 3. Konfigurace databÃ¡ze

```python
# config.py (vÃ½chozÃ­ SQLite)
DB_URI = "sqlite:///data/project_infinit.db"

# Pro PostgreSQL:
DB_URI = "postgresql+psycopg2://user:password@localhost/project_infinit"
```

### 4. SpuÅ¡tÄ›nÃ­ pipeline

```bash
# KompletnÃ­ ETL pipeline
python main.py

# JednotlivÃ© komponenty
scrapy runspider extracting/rss_spider.py
python -c "from main import process_academic_documents; process_academic_documents()"
```

## ğŸ‡¨ğŸ‡¿ MÃ¡m shluk pÅ™ehledağŸ”„ Kroky ETL Pipeline (CZ)

1. **SbÄ›r dat** (`run_spiders()`)
   - RSS feedy z 12+ specializaÄ¯ a mainstream zdrojÅ¯
   - REST API (Wikipedia, SOCCAS Encyklopedie)
   - SociÃ¡lnÃ­ mÃ©dia (Reddit, X/Twitter)
   - Web scraping (Google News, Medium, Seznam)
   - AkademickÃ© dokumenty (PDF, DOC, DOCX z `academic_data/`)

2. **Import CSV** (`process_csv()`)
   - NaÄtenÃ­ scraped CSV souborÅ¯ z `export/csv/`
   - Deduplicita po URL a obsahu (SHA256)
   - Import validnÃ­ch zdrojÅ¯ do databÃ¡ze

3. **ZpracovÃ¡nÃ­ akademickÃ½ch dokumentÅ¯** (`process_academic_documents()`)
   - Extrakce textu z PDF (PyMuPDF), DOC/DOCX (python-docx)
   - Konverze legacy .doc na .docx pÅ™es LibreOffice
   - Validace obsahu (min 50 slov + nÃ¡boÅ¾skÃ¡ klÃ­ÄovÃ¡ slova)
   - MatchÃ¡nÃ­ dokumentÅ¯ na znÃ¡mÃ¡ hnutÃ­
   - Import do databÃ¡ze s metadaty

4. **Extrakce entit** (`process_entities()`)
   - NER: Extrakce osob, organizaÄÃ­, mÃ­st z obsahu
   - VytvoÅ™enÃ­ Alias zÃ¡znamÅ¯ pro varianty nÃ¡zvÅ¯ hnutÃ­
   - VytvoÅ™enÃ­ Location zÃ¡znamÅ¯ pro geografickÃ© odkazy
   - MatchÃ¡nÃ­ entit na databÃ¡zi hnutÃ­

5. **NLP analÃ½za** (`run_nlp()`)
   - **Detekce jazyka**: AutomatickÃ¡ identifikace ÄeskÃ½ch/anglickÃ½ch textÅ¯
   - **AnalÃ½za sentimentu**: SkÃ³rÃ¡nÃ­ emocionÃ¡lnÃ­ho tÃ³nu (kladnÃ©/zÃ¡pornÃ©/neutrÃ¡lnÃ­)
   - **Lemmatizace & POS tagging**: PÅ™es Stanza pipeline
   - GenerÃ¡nÃ­ sentiment logÅ¯ a zpravy analÃ½zy

6. **UklÃ¡dÃ¡nÃ­ & Export** (`load_scraped_csvs()`)
   - UchovÃ¡nÃ­ vÅ¡ech zpracovanÃ½ch dat v PostgreSQL/SQLite
   - GenerÃ¡nÃ­ CSV exportÅ¯ pro Power BI
   - VytvoÅ™enÃ­ komplexnÃ­ch audit logÅ¯
   - VytvÃ¡Å™enÃ­ analytickÃ½ch reportÅ¯

## ğŸ“Š Zdroje dat

Pipeline sbÃ­rÃ¡ data z vÃ­ce zdrojÅ¯ nakonfigurovanÃ½ch v `extracting/sources_config.yaml`:

| Typ          | Zdroj                       | Metoda        | Status             | Popis                              |
| ------------ | --------------------------- | ------------- | ------------------ | ---------------------------------- |
| RSS          | Sekty.TV                    | Feed Parser   | âœ… AktivnÃ­         | SpecializovanÃ© informace o sektÃ¡ch |
| RSS          | Sekty.cz                    | Feed Parser   | âœ… AktivnÃ­         | Novinky o nÃ¡boÅ¾enskÃ½ch hnutÃ­ch     |
| RSS          | Dingir.cz                   | Feed Parser   | âœ… AktivnÃ­         | AkademickÃ© nÃ¡boÅ¾enskÃ© studie       |
| RSS          | PastorÃ¡lnÃ­ pÃ©Äe             | Feed Parser   | âœ… AktivnÃ­         | PastoraÄnÃ­ pÃ©Äe                    |
| RSS          | Seznam ZprÃ¡vy               | Feed Parser   | âœ… AktivnÃ­         | ÄŒeskÃ½ zpravodajskÃ½ portÃ¡l          |
| RSS          | ÄŒeskÃ½ rozhlas (iRozhlas.cz) | Feed Parser   | âœ… AktivnÃ­         | VeÅ™ejnoprÃ¡vnÃ­ rozhlasovÃ© noviny    |
| RSS          | AktuÃ¡lnÄ›.cz                 | Feed Parser   | âœ… AktivnÃ­         | ÄŒeskÃ© zpravodajskÃ© strÃ¡nky         |
| RSS          | Forum24.cz                  | Feed Parser   | âœ… AktivnÃ­         | DiskuznÃ­ fÃ³rum                     |
| RSS          | DenÃ­k Alarm                 | Feed Parser   | âœ… AktivnÃ­         | InvestigativnÃ­ Å¾urnalistika        |
| RSS          | Blesk.cz                    | Feed Parser   | âœ… AktivnÃ­         | BulvÃ¡rnÃ­ noviny                    |
| Web          | Medium.seznam.cz            | Scrapy        | âœ… AktivnÃ­         | BlogovÃ© ÄlÃ¡nky                     |
| API          | SociologickÃ½ Ãºstav AVÄŒR     | MediaWiki API | âœ… AktivnÃ­         | AkademickÃ¡ vÃ½zkumnÃ¡ databÃ¡ze       |
| API          | Wikipedia (Czech)           | MediaWiki API | âœ… AktivnÃ­         | EncyklopedickÃ© ÄlÃ¡nky              |
| Search API   | Google News                 | Custom API    | â¸ï¸ Legacy          | Agregace novinek                   |
| SociÃ¡lnÃ­ API | Reddit                      | PRAW          | âœ… NakonfigurovÃ¡no | KomunitnÃ­ diskuze                  |
| SociÃ¡lnÃ­ API | X (Twitter)                 | Tweepy        | âœ… NakonfigurovÃ¡no | PÅ™Ã­spÄ›vky na sociÃ¡lnÃ­ch sÃ­tÃ­ch     |

## ğŸ§ª TestovÃ¡nÃ­

SpuÅ¡tÄ›nÃ­ komplexnÃ­ testovacÃ­ sady:

```bash
# Instalace testovacÃ­ch zÃ¡vislostÃ­
pip install pytest pytest-mock

# SpuÅ¡tÄ›nÃ­ vÅ¡ech testÅ¯
pytest testing/

# SpuÅ¡tÄ›nÃ­ specifickÃ©ho testu
pytest testing/test_nlp_analysis.py -v
```

## ğŸ“¦ ZÃ¡vislosti

### ZÃ¡kladnÃ­ poÅ¾adavky

- Python 3.10+
- Scrapy 2.13+
- SQLAlchemy 2.0+
- spaCy 3.7+
- transformers 4.52+
- pandas 2.3+

### API klienti

- praw 7.8+ (Reddit)
- tweepy 4.14+ (X/Twitter)
- requests 2.31+
- feedparser 6.0+

### ZpracovÃ¡nÃ­ dat

- PyMuPDF 1.23+ (PDF)
- openpyxl (Excel)
- fuzzywuzzy 0.18+ (porovnÃ¡vÃ¡nÃ­ textu)
- python-dotenv 1.0+ (prostÅ™edÃ­)

### VÃ½voj

- pytest (testovÃ¡nÃ­)
- pyright (type checking)

## ğŸ“Š VÃ½stupy

- **DatabÃ¡ze**: StrukturovanÃ¡ data v PostgreSQL/SQLite s vztahy
- **CSV exporty**: ZpracovanÃ¡ data v adresÃ¡Å™i `export/csv/`
- **Power BI**: PÅ™Ã­mÃ¡ integrace pÅ™es `export/to_powerbi.py`
- **AnalytickÃ© reporty**: NLP insights a entity vztahy
- **Logy**: KomplexnÃ­ logovÃ¡nÃ­ v `import_log.txt`

## ğŸ›¡ï¸ EtickÃ© zÃ¡sady

- âœ… RespektovÃ¡nÃ­ robots.txt a rate limitÅ¯
- âœ… SprÃ¡vnÃ¡ identifikace user agenta
- âœ… Minimalizace dat a ochrana soukromÃ­
- âœ… Atribuce zdrojÅ¯ a transparentnost
- âœ… AkademickÃ© a vÃ½zkumnÃ© zamÄ›Å™enÃ­ sbÄ›ru dat
- âœ… Å½Ã¡dnÃ½ sbÄ›r osobnÃ­ch dat bez souhlasu

## ğŸ”§ VÃ½voj

### Kvalita kÃ³du

- Type hinty v celÃ©m kÃ³du
- Pylance/Pyright type checking
- KomplexnÃ­ testovÃ© pokrytÃ­
- EtickÃ© scraping praktiky
- ModulÃ¡rnÃ­ architektura

### PÅ™idÃ¡nÃ­ novÃ½ch zdrojÅ¯

1. PÅ™idÃ¡nÃ­ konfigurace do `extracting/sources_config.yaml`
2. Implementace spideru v adresÃ¡Å™i `extracting/`
3. PÅ™idÃ¡nÃ­ testÅ¯ v adresÃ¡Å™i `testing/`
4. Aktualizace main.py orchestrace

### DatabÃ¡zovÃ© schÃ©ma

DatabÃ¡ze pouÅ¾Ã­vÃ¡ SQLAlchemy ORM s nÃ¡sledujÃ­cÃ­mi hlavnÃ­mi entitami:

- **Source**: ÄŒlÃ¡nky, pÅ™Ã­spÄ›vky a dokumenty
- **Movement**: NÃ¡boÅ¾enskÃ¡ hnutÃ­ a sekty
- **Alias**: AlternativnÃ­ nÃ¡zvy pro hnutÃ­
- **Location**: GeografickÃ© reference

## ğŸ“¬ BudoucÃ­ vÃ½voj

- [ ] VylepÅ¡enÃ© NLP modely pro ÄeÅ¡tinu
- [ ] MonitorovÃ¡nÃ­ sociÃ¡lnÃ­ch mÃ©diÃ­ v reÃ¡lnÃ©m Äase
- [ ] PokroÄilÃ¡ analÃ½za vztahÅ¯ entit
- [ ] GeografickÃ¡ vizualizace hnutÃ­
- [ ] AnalÃ½za trendÅ¯ a ÄasovÃ© Å™ady
- [ ] REST API pro pÅ™Ã­stup k datÅ¯m
- [ ] Web dashboard interface
- [ ] RozÅ¡Ã­Å™enÃ­ podpory vÃ­ce jazykÅ¯

---

Version: 2.1
Author: Adam Å eifert
License: MIT
Last updated: 2025-12-18

### Setting Up Social Media Sources

To enable Reddit and X (Twitter) data collection:

1. **Create `.env` file**

   ```bash
   cp .env.example .env
   ```

2. **Reddit API Setup**
   - Go to <https://www.reddit.com/prefs/apps>
   - Create a "script" application
   - Copy `client_id` and `client_secret` to `.env`:

     ```bash
     REDDIT_CLIENT_ID=your_client_id
     REDDIT_CLIENT_SECRET=your_client_secret
     REDDIT_USER_AGENT=ProjectInfinit/1.0 (by your_username)
     ```

3. **X/Twitter API Setup**
   - Register at <https://developer.twitter.com/>
   - Create an app with API v2 access
   - Copy Bearer Token to `.env`:

     ```bash
     X_BEARER_TOKEN=your_bearer_token
     ```

---

## Output Summary

- **Structured data:** PostgreSQL database
- **Processed files:** CSV exports in `export/csv/`
- **Dashboards:** Power BI visualization-ready
- **Reports:** Analysis results and statistics

---

**Version:** 2.1 | **Author:** Adam Seifert | **License:** MIT | **Updated:** 2025-02-13
