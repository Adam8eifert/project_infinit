# ğŸ“˜ Project Infinit - Analysis of New Religious Movements in the Czech Republic

An ETL pipeline for collecting, analyzing, and visualizing information about new religious movements in the Czech Republic. Features ethical web scraping, NLP analysis, fuzzy entity matching, and structured data storage.

**License:** GNU General Public License v3.0 (GPLv3)

[ğŸ‡¨ğŸ‡¿ ÄŒeskÃ¡ verze nÃ­Å¾e](#-projekt-infinit---analÃ½za-novÃ½ch-nÃ¡boÅ¾enskÃ½ch-hnutÃ­-v-Är)

## ğŸŒŸ Features

- Ethical web scraping with rate limiting and robots.txt respect
- Automated data collection from multiple sources:
  - RSS feeds from specialized websites
  - REST APIs (Wikipedia, SOCCAS)
  - Social media APIs (Reddit, X/Twitter)
  - Web scraping for news aggregators
- Natural Language Processing:
  - **Multilingual support**: Czech (Stanza) and English (Stanza) with automatic language detection
  - Named Entity Recognition via Hugging Face Transformers (WikiNeural, BioBERT)
  - Sentiment analysis via multilingual BERT models (nlptown/bert-base-multilingual)
  - **Fuzzy entity matching**: 70% similarity threshold with 4-tier matching (direct name â†’ direct alias â†’ fuzzy name â†’ fuzzy alias)
  - Movement classification and relationship analysis
  - Academic document processing (PDF, DOC, DOCX text extraction)
  - **139 known religious movements** with alias matching
- Structured data storage in PostgreSQL/SQLite
- Export capabilities for further analysis and Power BI integration
- Comprehensive testing suite with pytest
- Type-safe code with Pylance/Pyright integration

## ğŸ”§ Technology Stack

### Core Technologies

- **Python 3.10+** - Core programming language
- **Mamba/Conda** - Package and environment management
- **PostgreSQL** - Primary database (with SQLite support)

### Web Scraping

- **Scrapy 2.14+** - Web scraping framework with ethical settings
- **Scrapy-Playwright 0.0.46+** - JavaScript-rendered page scraping (iDNES archiv)
- **Feedparser 6.0+** - RSS/Atom feed parsing
- **Requests 2.31+** - HTTP library

### Natural Language Processing

- **Stanza 1.8+** - Czech language NLP pipeline (tokenization, POS, NER, lemmatization)
- **Hugging Face Transformers 4.52+** - Advanced NLP models for sentiment analysis
- **spaCy 3.7+** (legacy) - Alternative NLP toolkit

### Data Processing

- **SQLAlchemy 2.0+** - Database ORM with PostgreSQL/SQLite support
- **pandas 2.3+** - Data manipulation and CSV processing
- **PyMuPDF (fitz) 1.23+** - PDF text extraction
- **python-docx 1.1+** - Word document processing (.doc, .docx)
- **FuzzyWuzzy 0.18+** - Fuzzy string matching for entity resolution

### API Clients (Dependencies)

- **PRAW 7.8+** - Reddit API client
- **Tweepy 4.14+** - X (Twitter) API client

### Development & Testing

- **pytest** - Testing framework with mocking
- **Pyright/Pylance** - Type checking and IDE support
- **PyYAML** - Configuration management
- **python-dotenv 1.0+** - Environment variable management

## ğŸ—‚ï¸ Project Structure

```text
project_infinit/
â”œâ”€â”€ extracting/              # Web scrapers and configurations
â”‚   â”œâ”€â”€ sources_config.yaml    # Centralized source configuration
â”‚   â”œâ”€â”€ spider_settings.py     # Ethical scraping settings
â”‚   â”œâ”€â”€ keywords.py           # Keyword filtering utilities
â”‚   â”œâ”€â”€ config_loader.py      # YAML configuration loader
â”‚   â”œâ”€â”€ rss_spider.py        # Universal RSS feed scraper
â”‚   â”œâ”€â”€ api_spider.py        # Universal API scraper
â”‚   â”œâ”€â”€ social_media_spider.py # Social media API scraper
â”‚   â”œâ”€â”€ google_spider.py     # Google News scraper
â”‚   â”œâ”€â”€ medium_seznam_spider.py # Medium/Seznam scraper
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ processing/            # Data processing and analysis
â”‚   â”œâ”€â”€ nlp_analysis.py       # NLP pipeline with Czech support
â”‚   â”œâ”€â”€ import_csv_to_db.py   # CSV database ingestion utilities
â”‚   â”œâ”€â”€ import_pdf_to_db.py   # PDF processing and ingestion
â”‚   â”œâ”€â”€ manual_csv.py         # Manual CSV processing utilities
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ database/              # Database layer
â”‚   â”œâ”€â”€ db_loader.py          # SQLAlchemy models and connections
â”‚   â”œâ”€â”€ models/              # Database models
â”‚   â”‚   â”œâ”€â”€ source.py        # Source model
â”‚   â”‚   â”œâ”€â”€ movement.py      # Movement model
â”‚   â”‚   â”œâ”€â”€ alias.py         # Alias model
â”‚   â”‚   â”œâ”€â”€ location.py      # Location model
â”‚   â”‚   â”œâ”€â”€ source_quality.py # Source quality model
â”‚   â”‚   â”œâ”€â”€ geographic_analysis.py # Geographic analysis model
â”‚   â”‚   â”œâ”€â”€ temporal_analysis.py # Temporal analysis model
â”‚   â”‚   â””â”€â”€ __init__.py      # Models init
â”‚   â”œâ”€â”€ schema.sql           # Database schema
â”‚   â”œâ”€â”€ views.sql            # Database views
â”‚   â”œâ”€â”€ deduplicate_sources.py # Source deduplication utilities
â”‚   â”œâ”€â”€ ANALYTICS_README.md  # Analytics documentation
â”‚   â”œâ”€â”€ DEDUPLICATION_README.md # Deduplication documentation
â”‚   â”œâ”€â”€ migrate_analytics.py # Analytics migration script
â”‚   â”œâ”€â”€ migrations/          # Database migrations
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ testing/               # Test suite
â”‚   â”œâ”€â”€ test_*.py           # Unit tests for all modules
â”‚   â”œâ”€â”€ conftest.py         # Pytest configuration
â”‚   â”œâ”€â”€ README.md           # Testing documentation
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ export/                # Output files and exports
â”‚   â”œâ”€â”€ csv/               # Scraped and processed CSV data
â”‚   â””â”€â”€ to_powerbi.py      # Power BI export utilities
â”œâ”€â”€ data/                  # Application data directory
â”œâ”€â”€ academic_data/         # Academic documents (PDF, DOC, DOCX)
â”‚   â””â”€â”€ README.md          # Academic data documentation
â”œâ”€â”€ csv_manual/            # Manual CSV imports
â”‚   â””â”€â”€ README.md          # Manual CSV documentation
â”œâ”€â”€ nnh-db/                # Docker database setup
â”‚   â”œâ”€â”€ docker             # Docker files
â”‚   â””â”€â”€ docker-compose.yml # Docker Compose configuration
â”œâ”€â”€ .github/               # GitHub configuration
â”‚   â””â”€â”€ copilot-instructions.md # AI assistant instructions
â”œâ”€â”€ config.py              # Database and app configuration
â”œâ”€â”€ config_loader.py       # Configuration loader utility
â”œâ”€â”€ csv_utils.py           # CSV utility functions
â”œâ”€â”€ main.py                # Main ETL orchestrator
â”œâ”€â”€ seed_movements.py      # Seed movements and aliases utility
â”œâ”€â”€ environment.yml        # Conda/Mamba environment
â”œâ”€â”€ pyrightconfig.json    # Pyright type checking config
â”œâ”€â”€ .gitignore             # Git ignore patterns
â”œâ”€â”€ LICENSE                # Project license
â”œâ”€â”€ SOCIAL_MEDIA_SETUP.md  # Social media API setup guide
â””â”€â”€ readme.md             # This file
```

## ğŸš€ Quick Start

### 1. Clone and Setup Environment

#### Option A: Using Mamba/Conda (Recommended)

```bash
git clone https://github.com/Adam8eifert/project_infinit.git
cd project_infinit

# Create Conda environment from environment.yml
mamba create -n project_infinit -y --file environment.yml
# or with conda:
# conda env create -f environment.yml

# Activate environment
mamba activate project_infinit
# or: conda activate project_infinit
```

**Note:** This project uses Conda/Mamba exclusively. Traditional pip/venv setup is not supported due to complex dependencies (Playwright, Stanza models, etc.).

### 2. Setup NLP Models and Dependencies

```bash
# Download Stanza Czech model (primary NLP pipeline)
python -c "import stanza; stanza.download('cs')"

# Optional: Download spaCy Czech model (legacy support)
python -m spacy download cs_core_news_md
```

### 3. Configure Database

The project supports both PostgreSQL and SQLite.

```python
# config.py (default configuration)
DB_URI = "sqlite:///data/project_infinit.db"
```

For PostgreSQL:

```python
DB_URI = "postgresql+psycopg2://username:password@localhost/nsm_db"
```

### 4. Configure Social Media APIs (Optional)

To enable Reddit and X (Twitter) data collection:

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your API keys (see .env.example for instructions)
```

### 5. Run the Pipeline

```bash
# Run complete ETL pipeline
python main.py

# Or run individual components
scrapy runspider extracting/rss_spider.py
scrapy runspider extracting/api_spider.py
scrapy runspider extracting/social_media_spider.py
```

## ğŸ”„ ETL Pipeline Steps

1. **Data Collection** (`run_spiders()`)
   - RSS feeds from 12+ specialized and mainstream sources
   - REST APIs (Wikipedia, SOCCAS Encyclopedia)
   - Social media posts from Reddit and X/Twitter
   - Web scraping for news aggregators (Google News, Medium, Seznam)
   - Academic documents (PDF, DOC, DOCX from `academic_data/`)

2. **CSV Import** (`process_csv()`)
   - Load scraped CSV files from `export/csv/`
   - Deduplicate by URL and content hash (SHA256)
   - Import validated sources to database

3. **Academic Document Processing** (`process_academic_documents()`)
   - Extract text from PDF (PyMuPDF), DOC/DOCX (python-docx)
   - Convert legacy .doc to .docx via LibreOffice
   - Validate content (min 50 words + religious keywords)
   - Match documents to known movements
   - Import to database with metadata

4. **Entity Extraction** (`process_entities()`)
   - NER: Extract persons, organizations, locations from content
   - Create Alias records for movement name variations
   - Create Location records for geographic references
   - Match entities to movement database

5. **NLP Analysis** (`run_nlp()`)
   - **Language detection**: Automatically identify Czech/English
   - **Sentiment analysis**: Score emotional tone (positive/negative/neutral)
   - **Lemmatization & POS tagging**: Via Stanza pipelines
   - Generate sentiment logs and analysis reports

6. **Data Storage & Export** (`load_scraped_csvs()`)
   - Persist all processed data to PostgreSQL/SQLite
   - Generate CSV exports for Power BI
   - Create comprehensive audit logs

## ğŸ“Š Data Sources

The pipeline collects data from multiple sources configured in `extracting/sources_config.yaml`:

| Type       | Source                      | Method        | Status        | Description                  |
| ---------- | --------------------------- | ------------- | ------------- | ---------------------------- |
| RSS        | Sekty.TV                    | Feed Parser   | âœ… Active     | Specialized sect information |
| RSS        | Sekty.cz                    | Feed Parser   | âœ… Active     | Religious movement news      |
| RSS        | Dingir.cz                   | Feed Parser   | âœ… Active     | Academic religious studies   |
| RSS        | PastorÃ¡lnÃ­ pÃ©Äe             | Feed Parser   | âœ… Active     | Pastoral care resources      |
| RSS        | Seznam ZprÃ¡vy               | Feed Parser   | âœ… Active     | Czech news portal            |
| RSS        | iDNES.cz - DomÃ¡cÃ­           | Feed Parser   | âœ… Active     | Czech mainstream news        |
| RSS        | ÄŒeskÃ½ rozhlas (iRozhlas.cz) | Feed Parser   | âœ… Active     | Public radio news            |
| RSS        | AktuÃ¡lnÄ›.cz                 | Feed Parser   | âœ… Active     | Czech news website           |
| RSS        | Forum24.cz                  | Feed Parser   | âœ… Active     | Discussion forum             |
| RSS        | DenÃ­k Alarm                 | Feed Parser   | âœ… Active     | Investigative journalism     |
| RSS        | Blesk.cz                    | Feed Parser   | âœ… Active     | Tabloid news                 |
| Web        | iDNES archiv (Playwright)   | Scrapy        | ğŸš§ Blocked    | Sekty-kulty-mesiÃ¡Å¡i section  |
| Web        | Medium.seznam.cz            | Scrapy        | âœ… Active     | Blog articles                |
| API        | SociologickÃ½ Ãºstav AVÄŒR     | MediaWiki API | âœ… Active     | Academic research database   |
| API        | Wikipedia (Czech)           | MediaWiki API | âœ… Active     | Encyclopedia articles        |
| Search API | Google News                 | Custom API    | â¸ï¸ Legacy     | News aggregation             |
| Social API | Reddit                      | PRAW          | âœ… Configured | Community discussions        |
| Social API | X (Twitter)                 | Tweepy        | âœ… Configured | Social media posts           |

## ğŸ§ª Testing

Run the comprehensive test suite:

```bash
# Test dependencies are in environment.yml
mamba activate project_infinit

# Run all tests
pytest testing/

# Run specific test
pytest testing/test_nlp_analysis.py -v

# Test fuzzy matching (70% threshold)
pytest testing/test_keywords.py -v
```

## ğŸ“¦ Dependencies

### Core Requirements

- Python 3.10.19+
- Scrapy 2.14+
- SQLAlchemy 2.0+
- Stanza 1.8+ (primary NLP)
- Hugging Face Transformers 4.52+
- pandas 2.3+

### NLP & Text Processing

- stanza 1.8+ (Primary: Czech/English NLP - tokenization, POS, NER, lemmatization)
- transformers 4.52+ (sentiment analysis: multilingual BERT)
- langdetect (automatic language detection)
- FuzzyWuzzy + Levenshtein (70% threshold fuzzy matching for 139 movements)
- spaCy 3.7+ (optional legacy support)

### API Clients

- praw 7.8+ (Reddit)
- tweepy 4.14+ (X/Twitter)
- requests 2.31+
- feedparser 6.0+

### Data Processing & NLP

- PyMuPDF (fitz) 1.23+ (PDF text extraction)
- python-docx 1.1+ (Word documents: .doc, .docx)
- openpyxl (Excel)
- fuzzywuzzy 0.18+ (fuzzy string matching)
- python-Levenshtein (fast string matching)
- python-dotenv 1.0+ (environment variables)

### Database

- psycopg2-binary 2.9+ (PostgreSQL adapter)
- SQLAlchemy 2.0+

### Development

- pytest 8.0+ (testing)
- pytest-mock (test mocking)
- pyright (type checking)
- pylance (IDE support)

## ğŸ“Š Outputs

- **Database**: Structured data in PostgreSQL/SQLite with relationships
- **CSV Exports**: Processed data in `export/csv/` directory
- **Power BI**: Direct integration via `export/to_powerbi.py`
- **Analysis Reports**: NLP insights and entity relationships
- **Logs**: Comprehensive logging in `import_log.txt`

## ğŸ›¡ï¸ Ethical Guidelines

- âœ… Respect robots.txt and rate limits
- âœ… Proper user agent identification
- âœ… Data minimization and privacy protection
- âœ… Source attribution and transparency
- âœ… Academic and research-focused data collection
- âœ… No personal data collection without consent

## ğŸ”§ Development

### Code Quality

- Type hints throughout codebase
- Pylance/Pyright type checking
- Comprehensive test coverage
- Ethical scraping practices
- Modular architecture

### Adding New Sources

1. Add configuration to `extracting/sources_config.yaml`
2. Implement spider in `extracting/` directory
3. Add tests in `testing/` directory
4. Update main.py orchestration

### Database Schema

The database uses SQLAlchemy ORM with the following main entities:

- **Source**: Articles, posts, and documents
- **Movement**: Religious movements and sects
- **Alias**: Alternative names for movements
- **Location**: Geographic references

## ğŸ“¬ Future Development

- [ ] Enhanced NLP models for Czech language
- [ ] Real-time social media monitoring
- [ ] Advanced entity relationship analysis
- [ ] Geographic visualization of movements
- [ ] Trend analysis and time series
- [ ] REST API for data access
- [ ] Web dashboard interface
- [ ] Multi-language support expansion

---

## ğŸ‡¨ğŸ‡¿ Projekt Infinit - AnalÃ½za novÃ½ch nÃ¡boÅ¾enskÃ½ch hnutÃ­ v ÄŒR

ETL pipeline pro sbÄ›r, analÃ½zu a vizualizaci informacÃ­ o novÃ½ch nÃ¡boÅ¾enskÃ½ch hnutÃ­ch v ÄŒeskÃ© republice. Zahrnuje etickÃ½ web scraping, NLP analÃ½zu a strukturovanÃ© uklÃ¡dÃ¡nÃ­ dat.

## ğŸŒŸ Funkce

- EtickÃ½ web scraping s omezenÃ­m rychlosti a respektovÃ¡nÃ­m robots.txt
- AutomatizovanÃ½ sbÄ›r dat z vÃ­ce zdrojÅ¯:
  - RSS feedy ze specializovanÃ½ch webÅ¯
  - REST API (Wikipedia, SOCCAS)
  - SociÃ¡lnÃ­ mÃ©dia API (Reddit, X/Twitter)
  - Web scraping pro news agregÃ¡tory
- ZpracovÃ¡nÃ­ pÅ™irozenÃ©ho jazyka:
  - Podpora ÄeÅ¡tiny pÅ™es spaCy
  - RozpoznÃ¡vÃ¡nÃ­ entit pÅ™es Hugging Face Transformers
  - AnalÃ½za sentimentu pÅ™es multijazyÄnÃ© BERT modely
  - **Fuzzy matching entit**: PrahovÃ¡ hodnota 70% s 4-ÃºrovÅˆovÃ½m matchovÃ¡nÃ­m
  - Klasifikace hnutÃ­ a analÃ½za vztahÅ¯
  - **139 znÃ¡mÃ½ch nÃ¡boÅ¾enskÃ½ch hnutÃ­** s aliasovÃ½m matchovÃ¡nÃ­m
- StrukturovanÃ© uklÃ¡dÃ¡nÃ­ dat v PostgreSQL/SQLite
- ExportnÃ­ moÅ¾nosti pro dalÅ¡Ã­ analÃ½zu a Power BI integraci
- KomplexnÃ­ testovacÃ­ sada s pytest
- Type-safe kÃ³d s Pylance/Pyright integracÃ­

## ğŸ”§ TechnologickÃ½ stack

### ZÃ¡kladnÃ­ technologie (detail)

- **Python 3.10+** - ZÃ¡kladnÃ­ programovacÃ­ jazyk
- **Mamba/Conda** - SprÃ¡va balÃ­ÄkÅ¯ a prostÅ™edÃ­
- **PostgreSQL** - PrimÃ¡rnÃ­ databÃ¡ze (s podporou SQLite)

### Web Scraping (CZ)

- **Scrapy 2.14+** - Framework pro web scraping s etickÃ½mi nastavenÃ­mi
- **Scrapy-Playwright 0.0.46+** - Scraping strÃ¡nek s JavaScriptem (iDNES archiv)
- **Feedparser 6.0+** - ParsovÃ¡nÃ­ RSS/Atom feedÅ¯
- **Requests 2.31+** - HTTP knihovna

### ZpracovÃ¡nÃ­ pÅ™irozenÃ©ho jazyka

- **Stanza 1.8+** - ÄŒeskÃ© NLP pipeline (tokenizace, POS, NER, lemmatizace)
- **Hugging Face Transformers 4.52+** - PokroÄilÃ© NLP modely pro analÃ½zu sentimentu
- **spaCy 3.7+** (legacy) - AlternativnÃ­ NLP toolkit

### ZpracovÃ¡nÃ­ dat

- **SQLAlchemy 2.0+** - Database ORM s podporou PostgreSQL/SQLite
- **pandas 2.3+** - Manipulace s daty a CSV zpracovÃ¡nÃ­
- **PyMuPDF (fitz) 1.23+** - Extrakce textu z PDF
- **python-docx 1.1+** - ZpracovÃ¡nÃ­ Word dokumentÅ¯ (.doc, .docx)
- **FuzzyWuzzy 0.18+ + Levenshtein** - Fuzzy porovnÃ¡vÃ¡nÃ­ Å™etÄ›zcÅ¯ (70% threshold pro 139 hnutÃ­)

### API klienti (detail)

- **PRAW 7.8+** - Reddit API klient
- **Tweepy 4.14+** - X (Twitter) API klient

### VÃ½voj a testovÃ¡nÃ­

- **pytest** - TestovacÃ­ framework s mocking
- **Pyright/Pylance** - Type checking a IDE podpora
- **PyYAML** - SprÃ¡va konfigurace
- **python-dotenv 1.0+** - SprÃ¡va environment promÄ›nnÃ½ch

## ğŸ—‚ï¸ Struktura projektu

```text
project_infinit/
â”œâ”€â”€ extracting/              # Web scrapery a konfigurace
â”‚   â”œâ”€â”€ sources_config.yaml    # CentralizovanÃ¡ konfigurace zdrojÅ¯
â”‚   â”œâ”€â”€ spider_settings.py     # EtickÃ¡ scraping nastavenÃ­
â”‚   â”œâ”€â”€ keywords.py           # Utility pro filtrovÃ¡nÃ­ klÃ­ÄovÃ½ch slov
â”‚   â”œâ”€â”€ config_loader.py      # NaÄÃ­tÃ¡nÃ­ YAML konfigurace
â”‚   â”œâ”€â”€ rss_spider.py        # UniverzÃ¡lnÃ­ RSS feed scraper
â”‚   â”œâ”€â”€ api_spider.py        # UniverzÃ¡lnÃ­ API scraper
â”‚   â”œâ”€â”€ social_media_spider.py # Social media API scraper
â”‚   â”œâ”€â”€ google_spider.py     # Google News scraper
â”‚   â”œâ”€â”€ medium_seznam_spider.py # Medium/Seznam scraper
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ processing/            # ZpracovÃ¡nÃ­ a analÃ½za dat
â”‚   â”œâ”€â”€ nlp_analysis.py       # NLP pipeline s podporou ÄeÅ¡tiny
â”‚   â”œâ”€â”€ import_csv_to_db.py   # Utility pro import CSV do databÃ¡ze
â”‚   â”œâ”€â”€ import_pdf_to_db.py   # ZpracovÃ¡nÃ­ a import PDF
â”‚   â”œâ”€â”€ manual_csv.py         # ManuÃ¡lnÃ­ zpracovÃ¡nÃ­ CSV
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ database/              # DatabÃ¡zovÃ¡ vrstva
â”‚   â”œâ”€â”€ db_loader.py          # SQLAlchemy modely a pÅ™ipojenÃ­
â”‚   â”œâ”€â”€ models/              # DatabÃ¡zovÃ© modely
â”‚   â”‚   â”œâ”€â”€ source.py        # Model zdrojÅ¯
â”‚   â”‚   â”œâ”€â”€ movement.py      # Model hnutÃ­
â”‚   â”‚   â”œâ”€â”€ alias.py         # Model aliasÅ¯
â”‚   â”‚   â””â”€â”€ location.py      # Model lokacÃ­
â”‚   â”œâ”€â”€ schema.sql           # DatabÃ¡zovÃ© schÃ©ma
â”‚   â”œâ”€â”€ views.sql            # DatabÃ¡zovÃ© views
â”‚   â”œâ”€â”€ migrations/          # DatabÃ¡zovÃ© migrace
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ testing/               # TestovacÃ­ sada
â”‚   â”œâ”€â”€ test_*.py           # Unit testy pro vÅ¡echny moduly
â”‚   â”œâ”€â”€ conftest.py         # Pytest konfigurace
â”‚   â””â”€â”€ README.md           # Dokumentace testovÃ¡nÃ­
â”œâ”€â”€ export/                # VÃ½stupnÃ­ soubory a exporty
â”‚   â”œâ”€â”€ csv/               # Scraped a zpracovanÃ¡ CSV data
â”‚   â””â”€â”€ to_powerbi.py      # Utility pro Power BI export
â”œâ”€â”€ data/                  # AplikaÄnÃ­ data
â”œâ”€â”€ academic_data/         # AkademickÃ© dokumenty (PDF, DOC, DOCX)
â”‚   â””â”€â”€ README.md          # Dokumentace akademickÃ½ch dat
â”œâ”€â”€ csv_manual/            # ManuÃ¡lnÃ­ CSV importy
â”‚   â””â”€â”€ README.md          # Dokumentace manuÃ¡lnÃ­ch CSV
â”œâ”€â”€ nnh-db/                # Docker databÃ¡zovÃ© nastavenÃ­
â”œâ”€â”€ .github/               # GitHub konfigurace
â”œâ”€â”€ config.py              # Konfigurace databÃ¡ze a aplikace
â”œâ”€â”€ main.py                # HlavnÃ­ ETL orchestrÃ¡tor
â”œâ”€â”€ seed_movements.py      # Utility pro seed hnutÃ­ a aliasÅ¯
â”œâ”€â”€ environment.yml        # Conda/Mamba prostÅ™edÃ­
â”œâ”€â”€ pyrightconfig.json    # Konfigurace Pyright type checking
â””â”€â”€ readme.md             # Tento soubor
```

## ğŸ‡¨ğŸ‡¿ RychlÃ½ start (CZ)

### 1a. KlonovÃ¡nÃ­ a pÅ™Ã­prava prostÅ™edÃ­ (CZ)

#### MoÅ¾nost A: PouÅ¾itÃ­ Mamba/Conda (DoporuÄeno)

```bash
git clone https://github.com/Adam8eifert/project_infinit.git
cd project_infinit

# VytvoÅ™enÃ­ Conda prostÅ™edÃ­ z environment.yml
mamba create -n project_infinit -y --file environment.yml
# nebo s conda:
# conda env create -f environment.yml

# Aktivace prostÅ™edÃ­
mamba activate project_infinit
# nebo: conda activate project_infinit
```

**PoznÃ¡mka:** Projekt vyuÅ¾Ã­vÃ¡ vÃ½hradnÄ› Conda/Mamba. TradiÄnÃ­ pip/venv setup nenÃ­ podporovÃ¡n kvÅ¯li komplexnÃ­m zÃ¡vislostem (Playwright, Stanza modely, atd.).

### 2. NastavenÃ­ NLP a zÃ¡vislostÃ­

Stanza automaticky stÃ¡hne ÄeskÃ© a anglickÃ© modely pÅ™i prvnÃ­m spuÅ¡tÄ›nÃ­:

```bash
# StaÅ¾enÃ­ Stanza ÄeskÃ©ho modelu (primÃ¡rnÃ­ NLP pipeline)
python -c "import stanza; stanza.download('cs')"

# VolitelnÃ©: StaÅ¾enÃ­ spaCy ÄeskÃ©ho modelu (legacy podpora)
python -m spacy download cs_core_news_md
```

### 3. Konfigurace databÃ¡ze

```python
# config.py (vÃ½chozÃ­ SQLite)
DB_URI = "sqlite:///data/project_infinit.db"

# Pro PostgreSQL:
DB_URI = "postgresql+psycopg2://user:password@localhost/project_infinit"
```

### 4. SpuÅ¡tÄ›nÃ­ pipeline

```bash
# KompletnÃ­ ETL pipeline
python main.py

# JednotlivÃ© komponenty
scrapy runspider extracting/rss_spider.py
python -c "from main import process_academic_documents; process_academic_documents()"
```

## ğŸ‡¨ğŸ‡¿ TechnologickÃ½ stack a architektura (CZ)

### ZÃ¡kladnÃ­ technologie

- **Python 3.10+** â€“ HlavnÃ­ jazyk
- **Stanza 1.11+** â€“ MultijazykovÃ½ NLP (ÄeÅ¡tina + angliÄtina) s automatickou detencÃ­ jazyka
- **langdetect** â€“ AutomatickÃ¡ detekce jazyka zdrojÅ¯
- **Transformers (HuggingFace)** â€“ Multilingual BERT pro sentiment analÃ½zu
- **SQLAlchemy 2.0+** â€“ ORM pro PostgreSQL/SQLite
- **Scrapy 2.13+** â€“ Framework pro web scraping (5 spiderÅ¯)
- **PyMuPDF 1.23+** â€“ Extrakce textu z PDF dokumentÅ¯
- **python-docx 1.1+** â€“ ÄŒtenÃ­ DOC/DOCX souborÅ¯

### Konfigurace

VeÅ¡kerÃ¡ konfigurace je centralizovÃ¡na v `extracting/sources_config.yaml`:

- Definice zdrojÅ¯ (RSS, API, webscraping)
- NLP klÃ­ÄovÃ¡ slova a vzory
- Known movements (75+ ÄeskÃ½ch NRM)
- Rate limiting a nastavenÃ­ scraperu

NaÄÃ­tÃ¡nÃ­ konfigurace probÃ­hÃ¡ pÅ™es `extracting/keywords.py` wrapper se automatickou validacÃ­.

## ğŸ‡¨ğŸ‡¿ Kroky ETL Pipeline (CZ)

1. **SbÄ›r dat** (`run_spiders()`)
   - RSS feedy z 12+ specializaÄ¯ a mainstream zdrojÅ¯
   - REST API (Wikipedia, SOCCAS Encyklopedie)
   - SociÃ¡lnÃ­ mÃ©dia (Reddit, X/Twitter)
   - Web scraping (Google News, Medium, Seznam)
   - AkademickÃ© dokumenty (PDF, DOC, DOCX z `academic_data/`)

2. **Import CSV** (`process_csv()`)
   - NaÄtenÃ­ scraped CSV souborÅ¯ z `export/csv/`
   - Deduplicita po URL a obsahu (SHA256)
   - Import validnÃ­ch zdrojÅ¯ do databÃ¡ze

3. **ZpracovÃ¡nÃ­ akademickÃ½ch dokumentÅ¯** (`process_academic_documents()`)
   - Extrakce textu z PDF (PyMuPDF), DOC/DOCX (python-docx)
   - Konverze legacy .doc na .docx pÅ™es LibreOffice
   - Validace obsahu (min 50 slov + nÃ¡boÅ¾skÃ¡ klÃ­ÄovÃ¡ slova)
   - MatchÃ¡nÃ­ dokumentÅ¯ na znÃ¡mÃ¡ hnutÃ­
   - Import do databÃ¡ze s metadaty

4. **Extrakce entit** (`process_entities()`)
   - NER: Extrakce osob, organizaÄÃ­, mÃ­st z obsahu
   - VytvoÅ™enÃ­ Alias zÃ¡znamÅ¯ pro varianty nÃ¡zvÅ¯ hnutÃ­
   - VytvoÅ™enÃ­ Location zÃ¡znamÅ¯ pro geografickÃ© odkazy
   - MatchÃ¡nÃ­ entit na databÃ¡zi hnutÃ­

5. **NLP analÃ½za** (`run_nlp()`)
   - **Detekce jazyka**: AutomatickÃ¡ identifikace ÄeskÃ½ch/anglickÃ½ch textÅ¯
   - **AnalÃ½za sentimentu**: SkÃ³rÃ¡nÃ­ emocionÃ¡lnÃ­ho tÃ³nu (kladnÃ©/zÃ¡pornÃ©/neutrÃ¡lnÃ­)
   - **Lemmatizace & POS tagging**: PÅ™es Stanza pipeline
   - GenerÃ¡nÃ­ sentiment logÅ¯ a zpravy analÃ½zy

6. **UklÃ¡dÃ¡nÃ­ & Export** (`load_scraped_csvs()`)
   - UchovÃ¡nÃ­ vÅ¡ech zpracovanÃ½ch dat v PostgreSQL/SQLite
   - GenerÃ¡nÃ­ CSV exportÅ¯ pro Power BI
   - VytvoÅ™enÃ­ komplexnÃ­ch audit logÅ¯
   - VytvÃ¡Å™enÃ­ analytickÃ½ch reportÅ¯

## ğŸ“Š Zdroje dat

Pipeline sbÃ­rÃ¡ data z vÃ­ce zdrojÅ¯ nakonfigurovanÃ½ch v `extracting/sources_config.yaml`:

| Typ          | Zdroj                       | Metoda        | Status             | Popis                              |
| ------------ | --------------------------- | ------------- | ------------------ | ---------------------------------- |
| RSS          | Sekty.TV                    | Feed Parser   | âœ… AktivnÃ­         | SpecializovanÃ© informace o sektÃ¡ch |
| RSS          | Sekty.cz                    | Feed Parser   | âœ… AktivnÃ­         | Novinky o nÃ¡boÅ¾enskÃ½ch hnutÃ­ch     |
| RSS          | Dingir.cz                   | Feed Parser   | âœ… AktivnÃ­         | AkademickÃ© nÃ¡boÅ¾enskÃ© studie       |
| RSS          | PastorÃ¡lnÃ­ pÃ©Äe             | Feed Parser   | âœ… AktivnÃ­         | PastoraÄnÃ­ pÃ©Äe                    |
| RSS          | Seznam ZprÃ¡vy               | Feed Parser   | âœ… AktivnÃ­         | ÄŒeskÃ½ zpravodajskÃ½ portÃ¡l          |
| RSS          | iDNES.cz - DomÃ¡cÃ­           | Feed Parser   | âœ… AktivnÃ­         | HlavnÃ­ ÄeskÃ© zprÃ¡vy                |
| RSS          | ÄŒeskÃ½ rozhlas (iRozhlas.cz) | Feed Parser   | âœ… AktivnÃ­         | VeÅ™ejnoprÃ¡vnÃ­ rozhlasovÃ© noviny    |
| RSS          | AktuÃ¡lnÄ›.cz                 | Feed Parser   | âœ… AktivnÃ­         | ÄŒeskÃ© zpravodajskÃ© strÃ¡nky         |
| RSS          | Forum24.cz                  | Feed Parser   | âœ… AktivnÃ­         | DiskuznÃ­ fÃ³rum                     |
| RSS          | DenÃ­k Alarm                 | Feed Parser   | âœ… AktivnÃ­         | InvestigativnÃ­ Å¾urnalistika        |
| RSS          | Blesk.cz                    | Feed Parser   | âœ… AktivnÃ­         | BulvÃ¡rnÃ­ noviny                    |
| Web          | iDNES archiv (Playwright)   | Scrapy        | ğŸš§ BlokovÃ¡no      | Sekce Sekty-kulty-mesiÃ¡Å¡i          |
| Web          | Medium.seznam.cz            | Scrapy        | âœ… AktivnÃ­         | BlogovÃ© ÄlÃ¡nky                     |
| API          | SociologickÃ½ Ãºstav AVÄŒR     | MediaWiki API | âœ… AktivnÃ­         | AkademickÃ¡ vÃ½zkumnÃ¡ databÃ¡ze       |
| API          | Wikipedia (Czech)           | MediaWiki API | âœ… AktivnÃ­         | EncyklopedickÃ© ÄlÃ¡nky              |
| Search API   | Google News                 | Custom API    | â¸ï¸ Legacy          | Agregace novinek                   |
| SociÃ¡lnÃ­ API | Reddit                      | PRAW          | âœ… NakonfigurovÃ¡no | KomunitnÃ­ diskuze                  |
| SociÃ¡lnÃ­ API | X (Twitter)                 | Tweepy        | âœ… NakonfigurovÃ¡no | PÅ™Ã­spÄ›vky na sociÃ¡lnÃ­ch sÃ­tÃ­ch     |

## ğŸ§ª TestovÃ¡nÃ­

SpuÅ¡tÄ›nÃ­ komplexnÃ­ testovacÃ­ sady:

```bash
# TestovacÃ­ zÃ¡vislosti jsou v environment.yml
mamba activate project_infinit

# SpuÅ¡tÄ›nÃ­ vÅ¡ech testÅ¯
pytest testing/

# SpuÅ¡tÄ›nÃ­ specifickÃ©ho testu
pytest testing/test_nlp_analysis.py -v

# Test fuzzy matchingu (70% threshold)
pytest testing/test_keywords.py -v
```

## ğŸ“¦ ZÃ¡vislosti

### ZÃ¡kladnÃ­ poÅ¾adavky

- Python 3.10.19+
- Scrapy 2.14+
- SQLAlchemy 2.0+
- Stanza 1.8+ (primÃ¡rnÃ­ NLP)
- Hugging Face Transformers 4.52+
- pandas 2.3+

### NLP & ZpracovÃ¡nÃ­ textu

- stanza 1.8+ (ÄeskÃ© NLP: tokenizace, POS, NER, lemmatizace)
- transformers 4.52+ (analÃ½za sentimentu: WikiNeuralNER)
- spaCy 3.7+ (legacy podpora)

### API klienti (minimÃ¡lnÃ­)

- praw 7.8+ (Reddit)
- tweepy 4.14+ (X/Twitter)
- requests 2.31+
- feedparser 6.0+

### ZpracovÃ¡nÃ­ dat (detail)

- PyMuPDF (fitz) 1.23+ (extrakce textu z PDF)
- python-docx 1.1+ (Word dokumenty: .doc, .docx)
- openpyxl (Excel)
- fuzzywuzzy 0.18+ (fuzzy porovnÃ¡vÃ¡nÃ­ Å™etÄ›zcÅ¯)
- python-Levenshtein (rychlÃ© porovnÃ¡vÃ¡nÃ­ Å™etÄ›zcÅ¯)
- python-dotenv 1.0+ (environment promÄ›nnÃ©)

### DatabÃ¡ze

- psycopg2-binary 2.9+ (PostgreSQL adapter)
- SQLAlchemy 2.0+

### VÃ½voj

- pytest 8.0+ (testovÃ¡nÃ­)
- pytest-mock (test mocking)
- pyright (type checking)
- pylance (IDE podpora)

## ğŸ“Š VÃ½stupy

- **DatabÃ¡ze**: StrukturovanÃ¡ data v PostgreSQL/SQLite s vztahy
- **CSV exporty**: ZpracovanÃ¡ data v adresÃ¡Å™i `export/csv/`
- **Power BI**: PÅ™Ã­mÃ¡ integrace pÅ™es `export/to_powerbi.py`
- **AnalytickÃ© reporty**: NLP insights a entity vztahy
- **Logy**: KomplexnÃ­ logovÃ¡nÃ­ v `import_log.txt`

## ğŸ›¡ï¸ EtickÃ© zÃ¡sady

- âœ… RespektovÃ¡nÃ­ robots.txt a rate limitÅ¯
- âœ… SprÃ¡vnÃ¡ identifikace user agenta
- âœ… Minimalizace dat a ochrana soukromÃ­
- âœ… Atribuce zdrojÅ¯ a transparentnost
- âœ… AkademickÃ© a vÃ½zkumnÃ© zamÄ›Å™enÃ­ sbÄ›ru dat
- âœ… Å½Ã¡dnÃ½ sbÄ›r osobnÃ­ch dat bez souhlasu

## ğŸ”§ VÃ½voj

### Kvalita kÃ³du

- Type hinty v celÃ©m kÃ³du
- Pylance/Pyright type checking
- KomplexnÃ­ testovÃ© pokrytÃ­
- EtickÃ© scraping praktiky
- ModulÃ¡rnÃ­ architektura

### PÅ™idÃ¡nÃ­ novÃ½ch zdrojÅ¯

1. PÅ™idÃ¡nÃ­ konfigurace do `extracting/sources_config.yaml`
2. Implementace spideru v adresÃ¡Å™i `extracting/`
3. PÅ™idÃ¡nÃ­ testÅ¯ v adresÃ¡Å™i `testing/`
4. Aktualizace main.py orchestrace

### DatabÃ¡zovÃ© schÃ©ma

DatabÃ¡ze pouÅ¾Ã­vÃ¡ SQLAlchemy ORM s nÃ¡sledujÃ­cÃ­mi hlavnÃ­mi entitami:

- **Source**: ÄŒlÃ¡nky, pÅ™Ã­spÄ›vky a dokumenty
- **Movement**: NÃ¡boÅ¾enskÃ¡ hnutÃ­ a sekty
- **Alias**: AlternativnÃ­ nÃ¡zvy pro hnutÃ­
- **Location**: GeografickÃ© reference

## ğŸ“¬ BudoucÃ­ vÃ½voj

- [ ] VylepÅ¡enÃ© NLP modely pro ÄeÅ¡tinu
- [ ] MonitorovÃ¡nÃ­ sociÃ¡lnÃ­ch mÃ©diÃ­ v reÃ¡lnÃ©m Äase
- [ ] PokroÄilÃ¡ analÃ½za vztahÅ¯ entit
- [ ] GeografickÃ¡ vizualizace hnutÃ­
- [ ] AnalÃ½za trendÅ¯ a ÄasovÃ© Å™ady
- [ ] REST API pro pÅ™Ã­stup k datÅ¯m
- [ ] Web dashboard interface
- [ ] RozÅ¡Ã­Å™enÃ­ podpory vÃ­ce jazykÅ¯

---

### Setting Up Social Media Sources

To enable Reddit and X (Twitter) data collection:

1. **Create `.env` file**

   ```bash
   cp .env.example .env
   ```

2. **Reddit API Setup**
   - Go to <https://www.reddit.com/prefs/apps>
   - Create a "script" application
   - Copy `client_id` and `client_secret` to `.env`:

     ```bash
     REDDIT_CLIENT_ID=your_client_id
     REDDIT_CLIENT_SECRET=your_client_secret
     REDDIT_USER_AGENT=ProjectInfinit/1.0 (by your_username)
     ```

3. **X/Twitter API Setup**
   - Register at <https://developer.twitter.com/>
   - Create an app with API v2 access
   - Copy Bearer Token to `.env`:

     ```bash
     X_BEARER_TOKEN=your_bearer_token
     ```

---

## Output Summary

- **Structured data:** PostgreSQL database
- **Processed files:** CSV exports in `export/csv/`
- **Dashboards:** Power BI visualization-ready
- **Reports:** Analysis results and statistics

---

**Version:** 2.3 | **Author:** Adam Seifert | **License:** GNU General Public License v3.0 | **Updated:** 2026-02-25
