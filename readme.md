# ğŸ“˜ Project Infinit - Analysis of New Religious Movements in the Czech Republic

An ETL pipeline for collecting, analyzing, and visualizing information about new religious movements and sects in the Czech Republic. Features ethical web scraping, NLP analysis, and structured data storage.

[ğŸ‡¨ğŸ‡¿ ÄŒeskÃ¡ verze nÃ­Å¾e](#-projekt-infinit---analÃ½za-novÃ½ch-nÃ¡boÅ¾enskÃ½ch-hnutÃ­-v-Är)

## ğŸŒŸ Features

- Ethical web scraping with rate limiting and robots.txt respect
- Automated data collection from multiple sources:
  - News websites and aggregators
  - Academic resources (Dingir.cz)
  - Wikipedia API
  - Specialized websites about religious movements
- Natural Language Processing:
  - Czech language support via Stanza
  - Named Entity Recognition for organizations and locations
  - Movement classification and relationship analysis
- Structured data storage in PostgreSQL
- Export capabilities for further analysis

## ğŸ”§ Technology Stack

- **Python 3.10+** - Core programming language
- **Scrapy** - Web scraping framework
- **Stanza** - NLP toolkit for Czech language
- **SQLAlchemy** - Database ORM
- **PostgreSQL** - Data storage
- **Apache Airflow** - ETL orchestration (optional)

---

# ğŸ‡¨ğŸ‡¿ Projekt Infinit - AnalÃ½za novÃ½ch nÃ¡boÅ¾enskÃ½ch hnutÃ­ v ÄŒR

ETL pipeline pro sbÄ›r, analÃ½zu a vizualizaci informacÃ­ o novÃ½ch nÃ¡boÅ¾enskÃ½ch hnutÃ­ch a sektÃ¡ch v ÄŒeskÃ© republice. Zahrnuje etickÃ½ web scraping, NLP analÃ½zu a strukturovanÃ© uklÃ¡dÃ¡nÃ­ dat.

## ğŸ—‚ï¸ Project Structure

```bash
project_infinit/
â”œâ”€â”€ scraping/           # Web scrapers and settings
â”‚   â”œâ”€â”€ spider_settings.py     # Ethical scraping config
â”‚   â”œâ”€â”€ keywords.py           # Centralized keyword management
â”‚   â””â”€â”€ *_spider.py          # Individual source scrapers
â”œâ”€â”€ processing/         # Data processing scripts
â”‚   â”œâ”€â”€ nlp_analysis.py      # NLP pipeline
â”‚   â”œâ”€â”€ data_cleaning.py     # Data validation
â”‚   â””â”€â”€ import_csv_to_db.py  # Database ingestion
â”œâ”€â”€ database/          # Database layer
â”‚   â”œâ”€â”€ db_loader.py         # SQLAlchemy models
â”‚   â””â”€â”€ schema.sql          # Database schema
â”œâ”€â”€ dags/              # Airflow DAGs (optional)
â”œâ”€â”€ export/            # Output files
â”‚   â””â”€â”€ csv/                 # Scraped/processed data
â”œâ”€â”€ data/              # Input data
â”‚   â”œâ”€â”€ pdf/                 # PDF documents
â”‚   â””â”€â”€ xlsx/               # Excel files
â”œâ”€â”€ config.py          # Configuration
â””â”€â”€ main.py            # Main ETL orchestrator
```

---

## ğŸš€ Quick Start

### 1. Clone and Setup Environment

```bash
git clone https://github.com/Adam8eifert/project_infinit.git
cd project_infinit
python -m venv venv

# Windows
./venv/Scripts/activate

# Linux/macOS
source venv/bin/activate
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt

# Download Czech language model for Stanza
python -m stanza.download cs
```

### 3. Configure Database

Create `config.py`:

```python
DB_URI = "postgresql+psycopg2://username:password@localhost/nsm_db"
```

### 4. Run Pipeline

```bash
# Run complete ETL pipeline
python main.py

# Or run individual spiders
scrapy runspider scraping/sekty_cz_spider.py
```

## ğŸ”„ Pipeline Steps

1. **Data Collection**
   - Scrape articles from configured RSS feeds and APIs
   - Collect posts from Reddit and X (Twitter)
   - Extract text from PDFs
   - Convert XLSX files to CSV

2. **Processing**
   - Clean and validate data
   - Perform NLP analysis
   - Extract entities and relationships

3. **Storage**
   - Import to SQLite database
   - Generate CSV exports
   - Update analysis results

### Data Sources

The pipeline collects data from multiple sources configured in `scraping/sources_config.yaml`:

| Type | Source | Method | Status |
|------|--------|--------|--------|
| RSS | Sekty.tv | Feed parser | âœ… Active |
| RSS | Sekty.cz | Feed parser | âœ… Active |
| RSS | Info Dingir | Feed parser | âœ… Active |
| RSS | Pastorace | Feed parser | âœ… Active |
| RSS | Medium/Seznam | Feed parser | âœ… Active |
| API | Wikipedia | MediaWiki API | âœ… Active |
| API | SOCCAS | REST API | âœ… Active |
| Social API | Reddit | Official API (PRAW) | âœ… Configured |
| Social API | X/Twitter | API v2 | âœ… Configured |
| Web | Google News | Web scraping | â¸ï¸ Legacy |

### Setting Up Social Media Sources

To enable Reddit and X (Twitter) data collection:

1. **Create `.env` file**
   ```bash
   cp .env.example .env
   ```

2. **Reddit API Setup**
   - Go to https://www.reddit.com/prefs/apps
   - Create a "script" application
   - Copy `client_id` and `client_secret` to `.env`:
     ```
     REDDIT_CLIENT_ID=your_client_id
     REDDIT_CLIENT_SECRET=your_client_secret
     REDDIT_USER_AGENT=ProjectInfinit/1.0 (by your_username)
     ```

3. **X/Twitter API Setup**
   - Register at https://developer.twitter.com/
   - Create an app with API v2 access
   - Copy Bearer Token to `.env`:
     ```
     X_BEARER_TOKEN=your_bearer_token
     ```

4. **Run Social Media Spiders**
   ```bash
   # Run all spiders including social media
   python main.py
   
   # Or run specific social media spider
   scrapy runspider scraping/social_media_spider.py
   ```

## ğŸ‡¨ğŸ‡¿ RychlÃ½ start

### 1. KlonovÃ¡nÃ­ a pÅ™Ã­prava prostÅ™edÃ­

```bash
git clone https://github.com/Adam8eifert/project_infinit.git
cd project_infinit
python -m venv venv

# Windows
./venv/Scripts/activate

# Linux/macOS
source venv/bin/activate
```

### 2. Instalace zÃ¡vislostÃ­

```bash
pip install -r requirements.txt

# StaÅ¾enÃ­ ÄeskÃ©ho jazykovÃ©ho modelu pro Stanza
python -m stanza.download cs
```

### 3. Konfigurace databÃ¡ze

DatabÃ¡ze se automaticky vytvoÅ™Ã­ pÅ™i prvnÃ­m spuÅ¡tÄ›nÃ­. PouÅ¾Ã­vÃ¡me SQLite (data/project_infinit.db).

```python
# config.py (standardnÄ›)
DB_URI = "sqlite:///data/project_infinit.db"
```

### 4. Konfigurace sociÃ¡lnÃ­ch mÃ©diÃ­

Chcete-li sbÃ­rat data z Redditu a X (Twitter):

1. **VytvoÅ™enÃ­ `.env` souboru**
   ```bash
   cp .env.example .env
   ```

2. **Reddit API Setup**
   - JdÄ›te na https://www.reddit.com/prefs/apps
   - VytvoÅ™te "script" aplikaci
   - ZkopÃ­rujte `client_id` a `client_secret` do `.env`:
     ```
     REDDIT_CLIENT_ID=vÃ¡Å¡_client_id
     REDDIT_CLIENT_SECRET=vÃ¡Å¡_client_secret
     REDDIT_USER_AGENT=ProjectInfinit/1.0 (od vaÅ¡eho_uÅ¾ivatele)
     ```

3. **X/Twitter API Setup**
   - Zaregistrujte se na https://developer.twitter.com/
   - VytvoÅ™te aplikaci s API v2 pÅ™Ã­stupem
   - ZkopÃ­rujte Bearer Token do `.env`:
     ```
     X_BEARER_TOKEN=vÃ¡Å¡_bearer_token
     ```

### 5. SpuÅ¡tÄ›nÃ­

```bash
# SpuÅ¡tÄ›nÃ­ celÃ©ho ETL pipeline
python main.py

# Nebo spuÅ¡tÄ›nÃ­ jednotlivÃ½ch spiderÅ¯
scrapy runspider scraping/sekty_cz_spider.py
```

## ğŸ”„ Kroky zpracovÃ¡nÃ­

1. **SbÄ›r dat**
   - Scraping ÄlÃ¡nkÅ¯ z nastavenÃ½ch zdrojÅ¯
   - Extrakce textu z PDF
   - Konverze XLSX souborÅ¯ do CSV

2. **ZpracovÃ¡nÃ­**
   - ÄŒiÅ¡tÄ›nÃ­ a validace dat
   - NLP analÃ½za
   - Extrakce entit a vztahÅ¯

3. **UklÃ¡dÃ¡nÃ­**
   - Import do PostgreSQL databÃ¡ze
   - GenerovÃ¡nÃ­ CSV exportÅ¯
   - Aktualizace vÃ½sledkÅ¯ analÃ½zy

---

## ğŸ“¦ Dependencies

### Core
- Python 3.10+
- Scrapy 2.11+
- SQLAlchemy 2.0+
- psycopg2-binary
- pandas
- stanza

### Processing
- openpyxl (Excel processing)
- PyMuPDF (PDF extraction)
- numpy
- scikit-learn

### Optional
- apache-airflow (DAG orchestration)
- jupyter (analysis notebooks)
- powerbi-client (visualization)

## ğŸ“Š Outputs

- Structured data in PostgreSQL
- CSV exports in `export/csv/`
- Power BI dashboards
- Analysis reports

## ğŸ›¡ï¸ Ethical Guidelines

- Respect robots.txt
- Rate limiting
- Proper user agent identification
- Data minimization
- Source attribution
- Privacy consideration

## ğŸ“¬ Future Development

- [ ] Additional source spiders
- [ ] Expanded format support
- [ ] Advanced NLP features
- [ ] Trend analysis
- [ ] Geographic visualization
- [ ] Timeline analysis
- [ ] API development

## ğŸ“¦ ZÃ¡vislosti

### ZÃ¡kladnÃ­
- Python 3.10+
- Scrapy 2.11+
- SQLAlchemy 2.0+
- psycopg2-binary
- pandas
- stanza

### ZpracovÃ¡nÃ­
- openpyxl (Excel)
- PyMuPDF (PDF)
- numpy
- scikit-learn

### VolitelnÃ©
- apache-airflow (DAG orchestrace)
- jupyter (analytickÃ© notebooky)
- powerbi-client (vizualizace)

## ğŸ“Š VÃ½stupy

- StrukturovanÃ¡ data v PostgreSQL
- CSV exporty v `export/csv/`
- Power BI dashboardy
- AnalytickÃ© reporty

## ï¿½ï¸ EtickÃ© zÃ¡sady

- RespektovÃ¡nÃ­ robots.txt
- OmezenÃ­ rychlosti
- SprÃ¡vnÃ¡ identifikace user agenta
- Minimalizace dat
- Atribuce zdrojÅ¯
- Ohled na soukromÃ­

## ğŸ“¬ BudoucÃ­ vÃ½voj

- [ ] DalÅ¡Ã­ zdrojovÃ© spidery
- [ ] RozÅ¡Ã­Å™enÃ¡ podpora formÃ¡tÅ¯
- [ ] PokroÄilÃ© NLP funkce
- [ ] AnalÃ½za trendÅ¯
- [ ] GeografickÃ¡ vizualizace
- [ ] ÄŒasovÃ¡ analÃ½za
- [ ] VÃ½voj API

---

Version: 2.0
Author: Adam Å eifert
License: MIT
Last updated: 2025-11-08

