# ğŸ“˜ Project Infinit - Analysis of New Religious Movements in the Czech Republic

An ETL pipeline for collecting, analyzing, and visualizing information about new religious movements and sects in the Czech Republic. Features ethical web scraping, NLP analysis, and structured data storage.

[ğŸ‡¨ğŸ‡¿ ÄŒeskÃ¡ verze nÃ­Å¾e](#-projekt-infinit---analÃ½za-novÃ½ch-nÃ¡boÅ¾enskÃ½ch-hnutÃ­-v-Är)

## ğŸŒŸ Features

- Ethical web scraping with rate limiting and robots.txt respect
- Automated data collection from multiple sources:
  - RSS feeds from specialized websites
  - REST APIs (Wikipedia, SOCCAS)
  - Social media APIs (Reddit, X/Twitter)
  - Web scraping for news aggregators
- Natural Language Processing:
  - Czech language support via spaCy
  - Named Entity Recognition via Hugging Face Transformers
  - Sentiment analysis via multilingual BERT models
  - Movement classification and relationship analysis
- Structured data storage in PostgreSQL/SQLite
- Export capabilities for further analysis and Power BI integration
- Comprehensive testing suite with pytest
- Type-safe code with Pylance/Pyright integration

## ğŸ”§ Technology Stack

- **Python 3.10+** - Core programming language
- **Scrapy** - Web scraping framework with custom settings
- **spaCy** - NLP toolkit for Czech language processing
- **Hugging Face Transformers** - Advanced NLP models for NER and sentiment
- **SQLAlchemy** - Database ORM with PostgreSQL/SQLite support
- **PRAW** - Reddit API client
- **Tweepy** - X (Twitter) API client
- **pandas** - Data manipulation and CSV processing
- **pytest** - Testing framework with mocking
- **PyYAML** - Configuration management

## ğŸ—‚ï¸ Project Structure

```
project_infinit/
â”œâ”€â”€ extracting/              # Web scrapers and configurations
â”‚   â”œâ”€â”€ sources_config.yaml    # Centralized source configuration
â”‚   â”œâ”€â”€ spider_settings.py     # Ethical scraping settings
â”‚   â”œâ”€â”€ keywords.py           # Keyword filtering utilities
â”‚   â”œâ”€â”€ config_loader.py      # YAML configuration loader
â”‚   â”œâ”€â”€ rss_spider.py        # Universal RSS feed scraper
â”‚   â”œâ”€â”€ api_spider.py        # Universal API scraper
â”‚   â”œâ”€â”€ social_media_spider.py # Social media API scraper
â”‚   â”œâ”€â”€ google_spider.py     # Google News scraper
â”‚   â”œâ”€â”€ medium_seznam_spider.py # Medium/Seznam scraper
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ processing/            # Data processing and analysis
â”‚   â”œâ”€â”€ nlp_analysis.py       # NLP pipeline with Czech support
â”‚   â”œâ”€â”€ import_csv_to_db.py   # CSV database ingestion utilities
â”‚   â”œâ”€â”€ import_pdf_to_db.py   # PDF processing and ingestion
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ database/              # Database layer
â”‚   â”œâ”€â”€ db_loader.py          # SQLAlchemy models and connections
â”‚   â”œâ”€â”€ models/              # Database models
â”‚   â”‚   â”œâ”€â”€ source.py        # Source model
â”‚   â”‚   â”œâ”€â”€ movement.py      # Movement model
â”‚   â”‚   â”œâ”€â”€ alias.py         # Alias model
â”‚   â”‚   â”œâ”€â”€ location.py      # Location model
â”‚   â”‚   â”œâ”€â”€ source_quality.py # Source quality model
â”‚   â”‚   â”œâ”€â”€ geographic_analysis.py # Geographic analysis model
â”‚   â”‚   â”œâ”€â”€ temporal_analysis.py # Temporal analysis model
â”‚   â”‚   â””â”€â”€ __init__.py      # Models init
â”‚   â”œâ”€â”€ schema.sql           # Database schema
â”‚   â”œâ”€â”€ views.sql            # Database views
â”‚   â”œâ”€â”€ deduplicate_sources.py # Source deduplication utilities
â”‚   â”œâ”€â”€ ANALYTICS_README.md  # Analytics documentation
â”‚   â”œâ”€â”€ migrate_analytics.py # Analytics migration script
â”‚   â”œâ”€â”€ migrations/          # Database migrations
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ testing/               # Test suite
â”‚   â”œâ”€â”€ test_*.py           # Unit tests for all modules
â”‚   â”œâ”€â”€ README.md           # Testing documentation
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ export/                # Output files and exports
â”‚   â”œâ”€â”€ csv/               # Scraped and processed CSV data
â”‚   â””â”€â”€ to_powerbi.py      # Power BI export utilities
â”œâ”€â”€ data/                  # Input data directory
â”œâ”€â”€ academic_data/         # Academic PDF documents for processing
â”œâ”€â”€ nnh-db/                # Docker database setup
â”‚   â”œâ”€â”€ docker             # Docker files
â”‚   â””â”€â”€ docker-compose.yml # Docker Compose configuration
â”œâ”€â”€ .github/               # GitHub configuration
â”‚   â””â”€â”€ copilot-instructions.md # AI assistant instructions
â”œâ”€â”€ config.py              # Database and app configuration
â”œâ”€â”€ main.py                # Main ETL orchestrator
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ environment.yml        # Conda environment
â”œâ”€â”€ pyrightconfig.json    # Pyright type checking config
â”œâ”€â”€ LICENSE                # Project license
â”œâ”€â”€ SOCIAL_MEDIA_SETUP.md  # Social media API setup guide
â”œâ”€â”€ import_log.txt         # CSV import log
â”œâ”€â”€ pdf_import_log.txt     # PDF import log
â””â”€â”€ readme.md             # This file
```

## ğŸš€ Quick Start

### 1. Clone and Setup Environment

```bash
git clone https://github.com/Adam8eifert/project_infinit.git
cd project_infinit

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/macOS
# or
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt
```

### 2. Setup NLP Models

```bash
# Download Czech spaCy model
python -m spacy download cs_core_news_md
```

### 3. Configure Database

The project supports both PostgreSQL and SQLite. By default, it uses SQLite for simplicity.

```python
# config.py (default configuration)
DB_URI = "sqlite:///data/project_infinit.db"
```

For PostgreSQL:

```python
DB_URI = "postgresql+psycopg2://username:password@localhost/nsm_db"
```

### 4. Configure Social Media APIs (Optional)

To enable Reddit and X (Twitter) data collection:

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your API keys (see .env.example for instructions)
```

### 5. Run the Pipeline

```bash
# Run complete ETL pipeline
python main.py

# Or run individual components
scrapy runspider extracting/rss_spider.py
scrapy runspider extracting/api_spider.py
scrapy runspider extracting/social_media_spider.py
```

## ğŸ”„ Pipeline Steps

1. **Data Collection**

   - Scrape RSS feeds from specialized websites
   - Query REST APIs (Wikipedia, SOCCAS)
   - Collect posts from Reddit and X (Twitter)
   - Web scraping for news aggregators

2. **Data Processing**

   - Clean and validate collected data
   - Perform NLP analysis (tokenization, POS tagging, NER, sentiment)
   - Extract entities and relationships
   - Classify content and movements

3. **Data Storage**
   - Import processed data to database
   - Generate CSV exports for analysis
   - Update Power BI datasets
   - Create analysis reports

## ğŸ“Š Data Sources

The pipeline collects data from multiple sources configured in `extracting/sources_config.yaml`:

| Type       | Source                      | Method        | Status        | Description                  |
| ---------- | --------------------------- | ------------- | ------------- | ---------------------------- |
| RSS        | Sekty.TV                    | Feed Parser   | âœ… Active     | Specialized sect information |
| RSS        | Sekty.cz                    | Feed Parser   | âœ… Active     | Religious movement news      |
| RSS        | Dingir.cz                   | Feed Parser   | âœ… Active     | Academic religious studies   |
| RSS        | PastorÃ¡lnÃ­ pÃ©Äe             | Feed Parser   | âœ… Active     | Pastoral care resources      |
| RSS        | Seznam ZprÃ¡vy               | Feed Parser   | âœ… Active     | Czech news portal            |
| RSS        | ÄŒeskÃ½ rozhlas (iRozhlas.cz) | Feed Parser   | âœ… Active     | Public radio news            |
| RSS        | AktuÃ¡lnÄ›.cz                 | Feed Parser   | âœ… Active     | Czech news website           |
| RSS        | Forum24.cz                  | Feed Parser   | âœ… Active     | Discussion forum             |
| RSS        | DenÃ­k Alarm                 | Feed Parser   | âœ… Active     | Investigative journalism     |
| RSS        | Blesk.cz                    | Feed Parser   | âœ… Active     | Tabloid news                 |
| Web        | Medium.seznam.cz            | Scrapy        | âœ… Active     | Blog articles                |
| API        | SociologickÃ½ Ãºstav AVÄŒR     | MediaWiki API | âœ… Active     | Academic research database   |
| API        | Wikipedia (Czech)           | MediaWiki API | âœ… Active     | Encyclopedia articles        |
| Search API | Google News                 | Custom API    | â¸ï¸ Legacy     | News aggregation             |
| Social API | Reddit                      | PRAW          | âœ… Configured | Community discussions        |
| Social API | X (Twitter)                 | Tweepy        | âœ… Configured | Social media posts           |

## ğŸ§ª Testing

Run the comprehensive test suite:

```bash
# Install test dependencies
pip install pytest pytest-mock

# Run all tests
pytest testing/

# Run specific test
pytest testing/test_nlp_analysis.py -v
```

## ğŸ“¦ Dependencies

### Core Requirements

- Python 3.10+
- Scrapy 2.13+
- SQLAlchemy 2.0+
- spaCy 3.7+
- transformers 4.52+
- pandas 2.3+

### API Clients

- praw 7.8+ (Reddit)
- tweepy 4.14+ (X/Twitter)
- requests 2.31+
- feedparser 6.0+

### Data Processing

- PyMuPDF 1.23+ (PDF)
- openpyxl (Excel)
- fuzzywuzzy 0.18+ (text matching)
- python-dotenv 1.0+ (environment)

### Development

- pytest (testing)
- pyright (type checking)

## ğŸ“Š Outputs

- **Database**: Structured data in PostgreSQL/SQLite with relationships
- **CSV Exports**: Processed data in `export/csv/` directory
- **Power BI**: Direct integration via `export/to_powerbi.py`
- **Analysis Reports**: NLP insights and entity relationships
- **Logs**: Comprehensive logging in `import_log.txt`

## ğŸ›¡ï¸ Ethical Guidelines

- âœ… Respect robots.txt and rate limits
- âœ… Proper user agent identification
- âœ… Data minimization and privacy protection
- âœ… Source attribution and transparency
- âœ… Academic and research-focused data collection
- âœ… No personal data collection without consent

## ğŸ”§ Development

### Code Quality

- Type hints throughout codebase
- Pylance/Pyright type checking
- Comprehensive test coverage
- Ethical scraping practices
- Modular architecture

### Adding New Sources

1. Add configuration to `extracting/sources_config.yaml`
2. Implement spider in `extracting/` directory
3. Add tests in `testing/` directory
4. Update main.py orchestration

### Database Schema

The database uses SQLAlchemy ORM with the following main entities:

- **Source**: Articles, posts, and documents
- **Movement**: Religious movements and sects
- **Alias**: Alternative names for movements
- **Location**: Geographic references

## ğŸ“¬ Future Development

- [ ] Enhanced NLP models for Czech language
- [ ] Real-time social media monitoring
- [ ] Advanced entity relationship analysis
- [ ] Geographic visualization of movements
- [ ] Trend analysis and time series
- [ ] REST API for data access
- [ ] Web dashboard interface
- [ ] Multi-language support expansion

---

# ğŸ‡¨ğŸ‡¿ Projekt Infinit - AnalÃ½za novÃ½ch nÃ¡boÅ¾enskÃ½ch hnutÃ­ v ÄŒR

ETL pipeline pro sbÄ›r, analÃ½zu a vizualizaci informacÃ­ o novÃ½ch nÃ¡boÅ¾enskÃ½ch hnutÃ­ch a sektÃ¡ch v ÄŒeskÃ© republice. Zahrnuje etickÃ½ web scraping, NLP analÃ½zu a strukturovanÃ© uklÃ¡dÃ¡nÃ­ dat.

## ğŸŒŸ Funkce

- EtickÃ½ web scraping s omezenÃ­m rychlosti a respektovÃ¡nÃ­m robots.txt
- AutomatizovanÃ½ sbÄ›r dat z vÃ­ce zdrojÅ¯:
  - RSS feedy ze specializovanÃ½ch webÅ¯
  - REST API (Wikipedia, SOCCAS)
  - SociÃ¡lnÃ­ mÃ©dia API (Reddit, X/Twitter)
  - Web scraping pro news agregÃ¡tory
- ZpracovÃ¡nÃ­ pÅ™irozenÃ©ho jazyka:
  - Podpora ÄeÅ¡tiny pÅ™es spaCy
  - RozpoznÃ¡vÃ¡nÃ­ entit pÅ™es Hugging Face Transformers
  - AnalÃ½za sentimentu pÅ™es multijazyÄnÃ© BERT modely
  - Klasifikace hnutÃ­ a analÃ½za vztahÅ¯
- StrukturovanÃ© uklÃ¡dÃ¡nÃ­ dat v PostgreSQL/SQLite
- ExportnÃ­ moÅ¾nosti pro dalÅ¡Ã­ analÃ½zu a Power BI integraci
- KomplexnÃ­ testovacÃ­ sada s pytest
- Type-safe kÃ³d s Pylance/Pyright integracÃ­

## ğŸ”§ TechnologickÃ½ stack

- **Python 3.10+** - ZÃ¡kladnÃ­ programovacÃ­ jazyk
- **Scrapy** - Framework pro web scraping s vlastnÃ­mi nastavenÃ­mi
- **spaCy** - NLP toolkit pro zpracovÃ¡nÃ­ ÄeÅ¡tiny
- **Hugging Face Transformers** - PokroÄilÃ© NLP modely pro NER a sentiment
- **SQLAlchemy** - Database ORM s podporou PostgreSQL/SQLite
- **PRAW** - Reddit API klient
- **Tweepy** - X (Twitter) API klient
- **pandas** - Manipulace s daty a CSV zpracovÃ¡nÃ­
- **pytest** - TestovacÃ­ framework s mocking
- **PyYAML** - SprÃ¡va konfigurace

## ğŸ—‚ï¸ Struktura projektu

```
project_infinit/
â”œâ”€â”€ extracting/              # Web scrapery a konfigurace
â”‚   â”œâ”€â”€ sources_config.yaml    # CentralizovanÃ¡ konfigurace zdrojÅ¯
â”‚   â”œâ”€â”€ spider_settings.py     # EtickÃ¡ scraping nastavenÃ­
â”‚   â”œâ”€â”€ keywords.py           # Utility pro filtrovÃ¡nÃ­ klÃ­ÄovÃ½ch slov
â”‚   â”œâ”€â”€ config_loader.py      # NaÄÃ­tÃ¡nÃ­ YAML konfigurace
â”‚   â”œâ”€â”€ rss_spider.py        # UnivezÃ¡lnÃ­ RSS feed scraper
â”‚   â”œâ”€â”€ api_spider.py        # UnivezÃ¡lnÃ­ API scraper
â”‚   â”œâ”€â”€ social_media_spider.py # Social media API scraper
â”‚   â”œâ”€â”€ google_spider.py     # Google News scraper
â”‚   â”œâ”€â”€ medium_seznam_spider.py # Medium/Seznam scraper
â”‚   â””â”€â”€ __pycache__/         # Python bytecode
â”œâ”€â”€ processing/            # ZpracovÃ¡nÃ­ a analÃ½za dat
â”‚   â”œâ”€â”€ nlp_analysis.py       # NLP pipeline s podporou ÄeÅ¡tiny
â”‚   â””â”€â”€ import_csv_to_db.py   # Utility pro import do databÃ¡ze
â”œâ”€â”€ database/              # DatabÃ¡zovÃ¡ vrstva
â”‚   â”œâ”€â”€ db_loader.py          # SQLAlchemy modely a pÅ™ipojenÃ­
â”‚   â”œâ”€â”€ models/              # DatabÃ¡zovÃ© modely
â”‚   â”‚   â”œâ”€â”€ source.py        # Model zdrojÅ¯
â”‚   â”‚   â”œâ”€â”€ movement.py      # Model hnutÃ­
â”‚   â”‚   â”œâ”€â”€ alias.py         # Model aliasÅ¯
â”‚   â”‚   â””â”€â”€ location.py      # Model lokacÃ­
â”‚   â””â”€â”€ schema.sql           # DatabÃ¡zovÃ© schÃ©ma
â”œâ”€â”€ testing/               # TestovacÃ­ sada
â”‚   â”œâ”€â”€ test_*.py           # Unit testy pro vÅ¡echny moduly
â”‚   â””â”€â”€ README.md           # Dokumentace testovÃ¡nÃ­
â”œâ”€â”€ export/                # VÃ½stupnÃ­ soubory a exporty
â”‚   â”œâ”€â”€ csv/               # Scraped a zpracovanÃ¡ CSV data
â”‚   â””â”€â”€ to_powerbi.py      # Utility pro Power BI export
â”œâ”€â”€ data/                  # VstupnÃ­ data
â”‚   â”œâ”€â”€ pdf/               # PDF dokumenty pro zpracovÃ¡nÃ­
â”‚   â””â”€â”€ xlsx/              # Excel soubory pro konverzi
â”œâ”€â”€ dags/                  # Apache Airflow DAGs (volitelnÃ©)
â”œâ”€â”€ .github/               # GitHub konfigurace
â”œâ”€â”€ config.py              # Konfigurace databÃ¡ze a aplikace
â”œâ”€â”€ main.py                # HlavnÃ­ ETL orchestrÃ¡tor
â”œâ”€â”€ requirements.txt       # Python zÃ¡vislosti
â”œâ”€â”€ environment.yml        # Conda prostÅ™edÃ­
â”œâ”€â”€ pyrightconfig.json    # Konfigurace Pyright type checking
â””â”€â”€ readme.md             # Tento soubor
```

## ğŸš€ RychlÃ½ start

### 1. KlonovÃ¡nÃ­ a pÅ™Ã­prava prostÅ™edÃ­

```bash
git clone https://github.com/Adam8eifert/project_infinit.git
cd project_infinit

# VytvoÅ™enÃ­ virtuÃ¡lnÃ­ho prostÅ™edÃ­
python -m venv venv
source venv/bin/activate  # Linux/macOS
# nebo
# venv\Scripts\activate   # Windows

# Instalace zÃ¡vislostÃ­
pip install -r requirements.txt
```

### 2. NastavenÃ­ NLP modelÅ¯

```bash
# StaÅ¾enÃ­ ÄeskÃ©ho spaCy modelu
python -m spacy download cs_core_news_md
```

### 3. Konfigurace databÃ¡ze

Projekt podporuje PostgreSQL i SQLite. StandardnÄ› pouÅ¾Ã­vÃ¡ SQLite pro jednoduchost.

```python
# config.py (vÃ½chozÃ­ konfigurace)
DB_URI = "sqlite:///data/project_infinit.db"
```

Pro PostgreSQL:

```python
DB_URI = "postgresql+psycopg2://username:password@localhost/nsm_db"
```

### 4. Konfigurace sociÃ¡lnÃ­ch mÃ©diÃ­ API (volitelnÃ©)

Pro povolenÃ­ sbÄ›ru dat z Redditu a X (Twitter):

```bash
# ZkopÃ­rovÃ¡nÃ­ Å¡ablony prostÅ™edÃ­
cp .env.example .env

# Ãšprava .env s vaÅ¡imi API klÃ­Äi (viz .env.example pro instrukce)
```

### 5. SpuÅ¡tÄ›nÃ­ pipeline

```bash
# SpuÅ¡tÄ›nÃ­ kompletnÃ­ho ETL pipeline
python main.py

# Nebo spuÅ¡tÄ›nÃ­ jednotlivÃ½ch komponent
scrapy runspider extracting/rss_spider.py
scrapy runspider extracting/api_spider.py
scrapy runspider extracting/social_media_spider.py
```

## ğŸ”„ Kroky zpracovÃ¡nÃ­

1. **SbÄ›r dat**

   - Scraping RSS feedÅ¯ ze specializovanÃ½ch webÅ¯
   - DotazovÃ¡nÃ­ REST API (Wikipedia, SOCCAS)
   - SbÄ›r pÅ™Ã­spÄ›vkÅ¯ z Redditu a X (Twitter)
   - Web scraping pro news agregÃ¡tory

2. **ZpracovÃ¡nÃ­ dat**

   - ÄŒiÅ¡tÄ›nÃ­ a validace nasbÃ­ranÃ½ch dat
   - ProvedenÃ­ NLP analÃ½zy (tokenizace, POS tagging, NER, sentiment)
   - Extrakce entit a vztahÅ¯
   - Klasifikace obsahu a hnutÃ­

3. **UklÃ¡dÃ¡nÃ­ dat**
   - Import zpracovanÃ½ch dat do databÃ¡ze
   - GenerovÃ¡nÃ­ CSV exportÅ¯ pro analÃ½zu
   - Aktualizace Power BI datasetÅ¯
   - VytvÃ¡Å™enÃ­ analytickÃ½ch reportÅ¯

## ğŸ“Š Zdroje dat

Pipeline sbÃ­rÃ¡ data z vÃ­ce zdrojÅ¯ nakonfigurovanÃ½ch v `extracting/sources_config.yaml`:

| Typ          | Zdroj                       | Metoda        | Status             | Popis                              |
| ------------ | --------------------------- | ------------- | ------------------ | ---------------------------------- |
| RSS          | Sekty.TV                    | Feed Parser   | âœ… AktivnÃ­         | SpecializovanÃ© informace o sektÃ¡ch |
| RSS          | Sekty.cz                    | Feed Parser   | âœ… AktivnÃ­         | Novinky o nÃ¡boÅ¾enskÃ½ch hnutÃ­ch     |
| RSS          | Dingir.cz                   | Feed Parser   | âœ… AktivnÃ­         | AkademickÃ© nÃ¡boÅ¾enskÃ© studie       |
| RSS          | PastorÃ¡lnÃ­ pÃ©Äe             | Feed Parser   | âœ… AktivnÃ­         | PastoraÄnÃ­ pÃ©Äe                    |
| RSS          | Seznam ZprÃ¡vy               | Feed Parser   | âœ… AktivnÃ­         | ÄŒeskÃ½ zpravodajskÃ½ portÃ¡l          |
| RSS          | ÄŒeskÃ½ rozhlas (iRozhlas.cz) | Feed Parser   | âœ… AktivnÃ­         | VeÅ™ejnoprÃ¡vnÃ­ rozhlasovÃ© noviny    |
| RSS          | AktuÃ¡lnÄ›.cz                 | Feed Parser   | âœ… AktivnÃ­         | ÄŒeskÃ© zpravodajskÃ© strÃ¡nky         |
| RSS          | Forum24.cz                  | Feed Parser   | âœ… AktivnÃ­         | DiskuznÃ­ fÃ³rum                     |
| RSS          | DenÃ­k Alarm                 | Feed Parser   | âœ… AktivnÃ­         | InvestigativnÃ­ Å¾urnalistika        |
| RSS          | Blesk.cz                    | Feed Parser   | âœ… AktivnÃ­         | BulvÃ¡rnÃ­ noviny                    |
| Web          | Medium.seznam.cz            | Scrapy        | âœ… AktivnÃ­         | BlogovÃ© ÄlÃ¡nky                     |
| API          | SociologickÃ½ Ãºstav AVÄŒR     | MediaWiki API | âœ… AktivnÃ­         | AkademickÃ¡ vÃ½zkumnÃ¡ databÃ¡ze       |
| API          | Wikipedia (Czech)           | MediaWiki API | âœ… AktivnÃ­         | EncyklopedickÃ© ÄlÃ¡nky              |
| Search API   | Google News                 | Custom API    | â¸ï¸ Legacy          | Agregace novinek                   |
| SociÃ¡lnÃ­ API | Reddit                      | PRAW          | âœ… NakonfigurovÃ¡no | KomunitnÃ­ diskuze                  |
| SociÃ¡lnÃ­ API | X (Twitter)                 | Tweepy        | âœ… NakonfigurovÃ¡no | PÅ™Ã­spÄ›vky na sociÃ¡lnÃ­ch sÃ­tÃ­ch     |

## ğŸ§ª TestovÃ¡nÃ­

SpuÅ¡tÄ›nÃ­ komplexnÃ­ testovacÃ­ sady:

```bash
# Instalace testovacÃ­ch zÃ¡vislostÃ­
pip install pytest pytest-mock

# SpuÅ¡tÄ›nÃ­ vÅ¡ech testÅ¯
pytest testing/

# SpuÅ¡tÄ›nÃ­ specifickÃ©ho testu
pytest testing/test_nlp_analysis.py -v
```

## ğŸ“¦ ZÃ¡vislosti

### ZÃ¡kladnÃ­ poÅ¾adavky

- Python 3.10+
- Scrapy 2.13+
- SQLAlchemy 2.0+
- spaCy 3.7+
- transformers 4.52+
- pandas 2.3+

### API klienti

- praw 7.8+ (Reddit)
- tweepy 4.14+ (X/Twitter)
- requests 2.31+
- feedparser 6.0+

### ZpracovÃ¡nÃ­ dat

- PyMuPDF 1.23+ (PDF)
- openpyxl (Excel)
- fuzzywuzzy 0.18+ (porovnÃ¡vÃ¡nÃ­ textu)
- python-dotenv 1.0+ (prostÅ™edÃ­)

### VÃ½voj

- pytest (testovÃ¡nÃ­)
- pyright (type checking)

## ğŸ“Š VÃ½stupy

- **DatabÃ¡ze**: StrukturovanÃ¡ data v PostgreSQL/SQLite s vztahy
- **CSV exporty**: ZpracovanÃ¡ data v adresÃ¡Å™i `export/csv/`
- **Power BI**: PÅ™Ã­mÃ¡ integrace pÅ™es `export/to_powerbi.py`
- **AnalytickÃ© reporty**: NLP insights a entity vztahy
- **Logy**: KomplexnÃ­ logovÃ¡nÃ­ v `import_log.txt`

## ğŸ›¡ï¸ EtickÃ© zÃ¡sady

- âœ… RespektovÃ¡nÃ­ robots.txt a rate limitÅ¯
- âœ… SprÃ¡vnÃ¡ identifikace user agenta
- âœ… Minimalizace dat a ochrana soukromÃ­
- âœ… Atribuce zdrojÅ¯ a transparentnost
- âœ… AkademickÃ© a vÃ½zkumnÃ© zamÄ›Å™enÃ­ sbÄ›ru dat
- âœ… Å½Ã¡dnÃ½ sbÄ›r osobnÃ­ch dat bez souhlasu

## ğŸ”§ VÃ½voj

### Kvalita kÃ³du

- Type hinty v celÃ©m kÃ³du
- Pylance/Pyright type checking
- KomplexnÃ­ testovÃ© pokrytÃ­
- EtickÃ© scraping praktiky
- ModulÃ¡rnÃ­ architektura

### PÅ™idÃ¡nÃ­ novÃ½ch zdrojÅ¯

1. PÅ™idÃ¡nÃ­ konfigurace do `extracting/sources_config.yaml`
2. Implementace spideru v adresÃ¡Å™i `extracting/`
3. PÅ™idÃ¡nÃ­ testÅ¯ v adresÃ¡Å™i `testing/`
4. Aktualizace main.py orchestrace

### DatabÃ¡zovÃ© schÃ©ma

DatabÃ¡ze pouÅ¾Ã­vÃ¡ SQLAlchemy ORM s nÃ¡sledujÃ­cÃ­mi hlavnÃ­mi entitami:

- **Source**: ÄŒlÃ¡nky, pÅ™Ã­spÄ›vky a dokumenty
- **Movement**: NÃ¡boÅ¾enskÃ¡ hnutÃ­ a sekty
- **Alias**: AlternativnÃ­ nÃ¡zvy pro hnutÃ­
- **Location**: GeografickÃ© reference

## ğŸ“¬ BudoucÃ­ vÃ½voj

- [ ] VylepÅ¡enÃ© NLP modely pro ÄeÅ¡tinu
- [ ] MonitorovÃ¡nÃ­ sociÃ¡lnÃ­ch mÃ©diÃ­ v reÃ¡lnÃ©m Äase
- [ ] PokroÄilÃ¡ analÃ½za vztahÅ¯ entit
- [ ] GeografickÃ¡ vizualizace hnutÃ­
- [ ] AnalÃ½za trendÅ¯ a ÄasovÃ© Å™ady
- [ ] REST API pro pÅ™Ã­stup k datÅ¯m
- [ ] Web dashboard interface
- [ ] RozÅ¡Ã­Å™enÃ­ podpory vÃ­ce jazykÅ¯

---

Version: 2.1
Author: Adam Å eifert
License: MIT
Last updated: 2025-12-18

### Setting Up Social Media Sources

To enable Reddit and X (Twitter) data collection:

1. **Create `.env` file**

   ```bash
   cp .env.example .env
   ```

2. **Reddit API Setup**

   - Go to https://www.reddit.com/prefs/apps
   - Create a "script" application
   - Copy `client_id` and `client_secret` to `.env`:
     ```
     REDDIT_CLIENT_ID=your_client_id
     REDDIT_CLIENT_SECRET=your_client_secret
     REDDIT_USER_AGENT=ProjectInfinit/1.0 (by your_username)
     ```

3. **X/Twitter API Setup**

   - Register at https://developer.twitter.com/
   - Create an app with API v2 access
   - Copy Bearer Token to `.env`:
     ```
     X_BEARER_TOKEN=your_bearer_token
     ```

4. **Run Social Media Spiders**

   ```bash
   # Run all spiders including social media
   python main.py

   # Or run specific social media spider
   scrapy runspider extracting/social_media_spider.py
   ```

## ğŸ‡¨ğŸ‡¿ RychlÃ½ start

### 1. KlonovÃ¡nÃ­ a pÅ™Ã­prava prostÅ™edÃ­

```bash
git clone https://github.com/Adam8eifert/project_infinit.git
cd project_infinit
python -m venv venv

# Windows
./venv/Scripts/activate

# Linux/macOS
source venv/bin/activate
```

### 2. Instalace zÃ¡vislostÃ­

```bash
pip install -r requirements.txt

# StaÅ¾enÃ­ ÄeskÃ©ho jazykovÃ©ho modelu pro spaCy
python -m spacy download cs_core_news_md
```

### 3. Konfigurace databÃ¡ze

DatabÃ¡ze se automaticky vytvoÅ™Ã­ pÅ™i prvnÃ­m spuÅ¡tÄ›nÃ­. PouÅ¾Ã­vÃ¡me SQLite (data/project_infinit.db).

```python
# config.py (standardnÄ›)
DB_URI = "sqlite:///data/project_infinit.db"
```

### 4. Konfigurace sociÃ¡lnÃ­ch mÃ©diÃ­

Chcete-li sbÃ­rat data z Redditu a X (Twitter):

1. **VytvoÅ™enÃ­ `.env` souboru**

   ```bash
   cp .env.example .env
   ```

2. **Reddit API Setup**

   - JdÄ›te na https://www.reddit.com/prefs/apps
   - VytvoÅ™te "script" aplikaci
   - ZkopÃ­rujte `client_id` a `client_secret` do `.env`:
     ```
     REDDIT_CLIENT_ID=vÃ¡Å¡_client_id
     REDDIT_CLIENT_SECRET=vÃ¡Å¡_client_secret
     REDDIT_USER_AGENT=ProjectInfinit/1.0 (od vaÅ¡eho_uÅ¾ivatele)
     ```

3. **X/Twitter API Setup**
   - Zaregistrujte se na https://developer.twitter.com/
   - VytvoÅ™te aplikaci s API v2 pÅ™Ã­stupem
   - ZkopÃ­rujte Bearer Token do `.env`:
     ```
     X_BEARER_TOKEN=vÃ¡Å¡_bearer_token
     ```

### 5. SpuÅ¡tÄ›nÃ­

```bash
# SpuÅ¡tÄ›nÃ­ celÃ©ho ETL pipeline
python main.py

# Nebo spuÅ¡tÄ›nÃ­ jednotlivÃ½ch spiderÅ¯
scrapy runspider extracting/sekty_cz_spider.py
```

## ğŸ”„ Kroky zpracovÃ¡nÃ­

1. **SbÄ›r dat**

   - Scraping ÄlÃ¡nkÅ¯ z nastavenÃ½ch zdrojÅ¯
   - Extrakce textu z PDF
   - Konverze XLSX souborÅ¯ do CSV

2. **ZpracovÃ¡nÃ­**

   - ÄŒiÅ¡tÄ›nÃ­ a validace dat
   - NLP analÃ½za
   - Extrakce entit a vztahÅ¯

3. **UklÃ¡dÃ¡nÃ­**
   - Import do PostgreSQL databÃ¡ze
   - GenerovÃ¡nÃ­ CSV exportÅ¯
   - Aktualizace vÃ½sledkÅ¯ analÃ½zy

---

## ğŸ“¦ Dependencies

### Core

- Python 3.10+
- Scrapy 2.11+
- SQLAlchemy 2.0+
- psycopg2-binary
- pandas
- stanza

### Processing

- openpyxl (Excel processing)
- PyMuPDF (PDF extraction)
- numpy
- scikit-learn

### Optional

- apache-airflow (DAG orchestration)
- jupyter (analysis notebooks)
- powerbi-client (visualization)

## ğŸ“Š Outputs

- Structured data in PostgreSQL
- CSV exports in `export/csv/`
- Power BI dashboards
- Analysis reports

## ğŸ›¡ï¸ Ethical Guidelines

- Respect robots.txt
- Rate limiting
- Proper user agent identification
- Data minimization
- Source attribution
- Privacy consideration

## ğŸ“¬ Future Development

- [ ] Additional source spiders
- [ ] Expanded format support
- [ ] Advanced NLP features
- [ ] Trend analysis
- [ ] Geographic visualization
- [ ] Timeline analysis
- [ ] API development

## ğŸ“¦ ZÃ¡vislosti

### ZÃ¡kladnÃ­

- Python 3.10+
- Scrapy 2.11+
- SQLAlchemy 2.0+
- psycopg2-binary
- pandas
- spaCy
- transformers

### ZpracovÃ¡nÃ­

- openpyxl (Excel)
- PyMuPDF (PDF)
- numpy
- scikit-learn

### VolitelnÃ©

- apache-airflow (DAG orchestrace)
- jupyter (analytickÃ© notebooky)
- powerbi-client (vizualizace)

## ğŸ“Š VÃ½stupy

- StrukturovanÃ¡ data v PostgreSQL
- CSV exporty v `export/csv/`
- Power BI dashboardy
- AnalytickÃ© reporty

## ï¿½ï¸ EtickÃ© zÃ¡sady

- RespektovÃ¡nÃ­ robots.txt
- OmezenÃ­ rychlosti
- SprÃ¡vnÃ¡ identifikace user agenta
- Minimalizace dat
- Atribuce zdrojÅ¯
- Ohled na soukromÃ­

## ğŸ“¬ BudoucÃ­ vÃ½voj

- [ ] DalÅ¡Ã­ zdrojovÃ© spidery
- [ ] RozÅ¡Ã­Å™enÃ¡ podpora formÃ¡tÅ¯
- [ ] PokroÄilÃ© NLP funkce
- [ ] AnalÃ½za trendÅ¯
- [ ] GeografickÃ¡ vizualizace
- [ ] ÄŒasovÃ¡ analÃ½za
- [ ] VÃ½voj API

---

Version: 2.1
Author: Adam Seifert
License: MIT
Last updated: 2025-12-18
